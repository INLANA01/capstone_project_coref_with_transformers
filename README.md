# capstone_project_coref_with_transformers

Abstract: Coreference resolution is the task of automatically determining the chain of expressions in language that refers to the same entity or antecedent. In this project, we have explored transformer models, like the BERT model and its recent variants (e.g., ALBERT), and have adapted them for performing coreference resolution tasks on the GAP (Gender Ambiguous Pronoun) dataset and measured their performance. Moreover, a complete quantitative and qualitative analysis of the data was done to compare the performances of various transformer models, like vanilla transformer models, trained and fine-tuned BERT and its variant models, and models from literature surveys on languages demonstrating different pronominal complexity.

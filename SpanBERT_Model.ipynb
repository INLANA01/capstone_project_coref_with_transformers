{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpanBERT Model",
      "provenance": [],
      "collapsed_sections": [
        "T48uTOiWPwJY",
        "5S7QPDFkP0N_"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOiSkAHsFQZTlX5LSYP3EF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/INLANA01/capstone_project_coref_with_transformers/blob/main/SpanBERT_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S7QPDFkP0N_"
      },
      "source": [
        "# Mandar Joshi NLP\n",
        "### https://github.com/facebookresearch/SpanBERT\n",
        "### https://github.com/mandarjoshi90/SpanBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE8uUzVl1_J9"
      },
      "source": [
        "filename = \"optional-change-to-your-file.txt\"\n",
        "\n",
        "# text = [\n",
        "# \"Firefly is an American space Western drama television series which ran from 2002-2003, created by writer and director Joss Whedon, under his Mutant Enemy Productions label.\",\n",
        "# \"Whedon served as an executive producer, along with Tim Minear.\",\n",
        "# \"The series is set in the year 2517, after the arrival of humans in a new star system and follows the adventures of the renegade crew of Serenity, a 'Firefly-class' spaceship.\",\n",
        "# \"The ensemble cast portrays the nine characters who live on Serenity.\",\n",
        "# \"Whedon pitched the show as 'nine people looking into the blackness of space and seeing nine different things.'\",\n",
        "# \"The show explores the lives of a group of people, some of whom fought on the losing side of a civil war, who make a living on the fringes of society as part of the pioneer culture of their star system.\",\n",
        "# \"In this future, the only two surviving superpowers, the United States and China, fused to form the central federal government, called the Alliance, resulting in the fusion of the two cultures.\",\n",
        "# \"According to Whedon's vision, 'nothing will change in the future: technology will advance, but we will still have the same political, moral, and ethical problems as today.'\",\n",
        "# \"Firefly premiered in the U.S. on the Fox network on September 20, 2002.\",\n",
        "# \"By mid-December, Firefly had averaged 4.7 million viewers per episode and was 98th in Nielsen ratings.\",\n",
        "# \"It was canceled after 11 of the 14 produced episodes were aired.\",\n",
        "# \"Despite the relatively short life span of the series, it received strong sales when it was released on DVD and has large fan support campaigns.\",\n",
        "# \"It won a Primetime Emmy Award in 2003 for Outstanding Special Visual Effects for a Series.\",\n",
        "# \"TV Guide ranked the series at No. 5 on their 2013 list of 60 shows that were 'Cancelled Too Soon.'\",\n",
        "# \"The post-airing success of the show led Whedon and Universal Pictures to produce Serenity, a 2005 film which continues from the story of the series, and the Firefly franchise expanded to other media, including comics and a role-playing game.\",\n",
        "# ]\n",
        "# text = ['The £2-a-day drug that can cut heart attack threat.',\n",
        "#         'Thousands of heart attack patients are to benefit from a £2-a-day drug, after officials recommended it be given to more people for longer.',\n",
        "# 'Anti-clotting drug ticagrelor slashes the risk of repeat attacks for people with heart disease.',\n",
        "# 'The drug is already given for 12 months after a heart attack, reducing the risk of a stroke or another attack.',\n",
        "# 'NHS watchdog NICE has advised that it should be given to people for four years, to further reduce the risk of cardiovascular problems.',\n",
        "# 'But now NHS watchdog NICE has advised that it should be given to people for four years, to further reduce the risk of cardiovascular problems.',\n",
        "# 'Some 140,000 people have a heart attack in England every year, and a quarter of these go on to have another attack or a stroke.',\n",
        "# 'Heart attacks and strokes are caused by the build-up of fatty material in artery walls to form a plaque.',\n",
        "# 'If the plaque breaks apart it can cause a blood clot, blocking blood to the heart causing a heart attack.',\n",
        "# 'And if the clot dislodges it can travel in the blood stream and block blood flow to the brain, causing a stroke.',\n",
        "# 'People who have already have one attack are at a higher risk of having another.',\n",
        "# 'Ticagrelor, which is made by UK firm AstraZeneca and sold under the trade name Brilique, reduces this risk by making clots less likely.',\n",
        "# 'The draft NICE guidance, published today, recommends that people be given 90 mg of ticagrelor for 12 months, followed by 60mg along with aspirin twice a day for another three years.',\n",
        "# 'Professor Carole Longson, director at the NICE health technology evaluation centre, said: \"Despite the availability of effective secondary prevention treatments, as many as a quarter of people who have had a heart attack go on to have another heart attack or stroke - often with devastating consequences.',\n",
        "# 'Fear of a recurrence can have a significant negative impact on a persons quality of life.',\n",
        "# 'The evidence shows that ticagrelor, in combination with aspirin, is effective at reducing the risk of further heart attacks and strokes in people who have already had a heart attack.',\n",
        "# 'In provisionally recommending ticagrelor we are pleased to be able to increase the treatment options available to the many thousands of people who stand to benefit from it.',\n",
        "# 'Because information on the efficacy and safety of ticagrelor - particularly the risk of bleeding - beyond three years is limited, the draft guidance does not recommend treatment with it beyond that period.']\n",
        "# text = [\"Zoe Telford -- played the police officer girlfriend of Simon, Maggie.\",\n",
        "#         \"Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again.\",\n",
        "#         \"Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class.\",\n",
        "#         \"Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.\"]\n",
        "text = ['Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3 million, as the 38-year-old became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks.']\n",
        "# text = [\n",
        "#         'Das £2-pro Tag- Medikament, das Herzinfarktrisiko senken kann',\n",
        "# 'Tausende von Herzinfarktpatienten werden von einem £2-pro Tag-Medikament profitieren, nachdem von den Behörden empfohlen wurde, dieses mehr Leuten für einen längeren Zeitraum zu verabreichen.',\n",
        "# 'Das gerinnungshemmende Medikament Ticagrelor reduziert das Risiko wiederholter Herzinfarkte für Personen mit einer Herzerkrankung.',\n",
        "# 'Das Medikament wird bereits 12 Monate lang nach einem Herzinfarkt verabreicht, wodurch das Risiko eines Schlaganfalls oder eines weiteren Herzinfarkts reduziert wird.',\n",
        "# 'Die Regulierungsbehörde NICE des NHS hat eine vier Jahre lange Einnahme empfohlen, um das Risiko kardiovaskulärer Probleme weiter zu reduzieren.',\n",
        "# 'Die Regulierungsbehörde NICE des NHS hat eine vier Jahre lange Einnahme empfohlen, um das Risiko kardiovaskulärer Probleme weiter zu reduzieren.',\n",
        "# 'Rund 140.000 Leute erleiden jedes Jahr einen Herzinfarkt und ein Viertel davon erleiden einen weiteren Herzinfarkt oder einen Schlaganfall.',\n",
        "# 'Herzinfarkte und Schlaganfälle werden durch Ansammlungen fetthaltigen Materials in den Arterienwänden verursacht, die Beläge bilden.',\n",
        "# 'Wenn der Belag auseinanderbricht, kann es ein Blutgerinnsel verursachen, das den Blutfluss bis zum Herzen verstopft und dadurch einen Herzinfarkt verursacht.',\n",
        "# 'Wenn sich das Blutgerinnsel löst, kann es durch den Blutstrom fließen und den Blutfluss zum Gehirn verstopfen, was einen Schlaganfall verursacht.',\n",
        "# 'Personen, die bereits einen Herzinfarkt hatten, unterliegen einem höheren Risiko einen weiteren zu haben.',\n",
        "# 'Ticagrelor, das von dem britischen Unternehmen AstraZeneca hergestellt wird und unter dem Handelsnamen Brilique vertrieben wird, reduziert dieses Risiko, indem es die Bildung von Blutgerinnseln unwahrscheinlicher macht.',\n",
        "# 'Der Anleitungsentwurf von NICE, der heute veröffentlicht wurde, empfiehlt eine 12-monatige Einnahme von 90mg Ticagrelor, gefolgt von 60mg mit einer zweimal täglichen Einnahme von Aspirin für die nächsten drei Jahre.',\n",
        "# 'Professor Carole Longson, Direktorin des NICE Gesundheitstechnologieevaluierungszentrums sagte: \"Trotz der Verfügbarkeit von Sekundärprävention haben ein Viertel aller Personen, die einen Herzinfarkt erlitten haben, einen weiteren Herzinfarkt oder einen Schlaganfall - oft mit desaströsen Folgen.'\n",
        "# 'Die Angst vor einem erneuten Herzinfarkt kann erhebliche negative Auswirkungen auf die Lebensqualität einer Person haben.',\n",
        "# 'Die Erfahrung zeigt, dass Ticagrelor in Kombination mit Aspirin effektiv bei der Reduzierung weiterer Herzinfarkte und Schlaganfälle bei Leuten ist, die bereits einen Herzinfarkt hatten.',\n",
        "# 'Durch eine vorläufige Empfehlung von Ticagrelor freuen wir uns, dass wir in der Lage sind, verfügbare Behandlungsoptionen an tausende von Menschen zu erweitern, die davon profitieren können.',\n",
        "# 'Die Information über die Wirksamkeit und Sicherheit von Ticagrelor - vor allem das Blutungsrisiko - ist auf einen Zeitraum von bis zu drei Jahren beschränkt. Der Anleitungsentwurf empfiehlt keine Behandlung, die über diesen Zeitraum hinausgeht.',\n",
        "# ]\n",
        "\n",
        "if filename != \"optional-change-to-your-file.txt\":\n",
        "    data = [l.strip() for l in open(filename).readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSS3Wh4b1_wQ"
      },
      "source": [
        "genre = \"mz\"\n",
        "# The Ontonotes data for training the model contains text from several sources\n",
        "# of very different styles. You need to specify the most suitable one out of:\n",
        "# \"bc\": broadcast conversation\n",
        "# \"bn\": broadcast news\n",
        "# \"mz\": magazine\n",
        "# \"nw\": newswire\n",
        "# \"pt\": Bible text\n",
        "# \"tc\": telephone conversation\n",
        "# \"wb\": web data\n",
        "\n",
        "model_name = \"spanbert_base\"\n",
        "# model_name = \"spanbert_large\"\n",
        "# The fine-tuned model to use. Options are:\n",
        "# bert_base\n",
        "# spanbert_base\n",
        "# bert_large\n",
        "# spanbert_large"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC26C6FV2DTz",
        "outputId": "575e317e-192b-43f8-ca65-728e8b8c65fc"
      },
      "source": [
        "! git clone https://github.com/mandarjoshi90/coref.git\n",
        "%cd coref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'coref'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 734 (delta 2), reused 0 (delta 0), pack-reused 728\u001b[K\n",
            "Receiving objects: 100% (734/734), 4.17 MiB | 24.29 MiB/s, done.\n",
            "Resolving deltas: 100% (441/441), done.\n",
            "/content/coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doa8zdvT2G5x"
      },
      "source": [
        "! sed 's/MarkupSafe==1.0/MarkupSafe==1.1.1/; s/scikit-learn==0.19.1/scikit-learn==0.21/; s/scipy==1.0.0/scipy==1.6.2/' < requirements.txt > tmp\n",
        "! mv tmp requirements.txt\n",
        "\n",
        "! sed 's/.D.GLIBCXX.USE.CXX11.ABI.0//' < setup_all.sh  > tmp\n",
        "! mv tmp setup_all.sh \n",
        "! chmod u+x setup_all.sh "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvdHCVqv2LYS"
      },
      "source": [
        "import os\n",
        "os.environ['data_dir'] = \".\"\n",
        "os.environ['CHOSEN_MODEL'] = model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciaVSaJd2NP7",
        "outputId": "b1eef8fe-a389-4e99-f940-9c1ac00bfde1"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "! pip uninstall -y tensorflow\n",
        "! pip install -r requirements.txt --log install-log.txt -q\n",
        "! ./setup_all.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 10.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 28.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6MB 12.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 65.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 65.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 58.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 70.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 71.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 76.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 13.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.3MB 339kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 63.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 66.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 430kB 64.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 256kB 73.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 58.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 55.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.7MB 22.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 27.4MB 100.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 72.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 491kB 71.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 377.1MB 38kB/s \n",
            "\u001b[K     |████████████████████████████████| 748.9MB 22kB/s \n",
            "\u001b[K     |████████████████████████████████| 8.8MB 35.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 327kB 71.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 256kB 70.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1MB 51.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25h  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for h5py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mmh3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for msgpack-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for psycopg2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-metadata 0.29.0 has requirement absl-py<0.13,>=0.9, but you'll have absl-py 0.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyasn1-modules 0.2.8 has requirement pyasn1<0.5.0,>=0.4.6, but you'll have pyasn1 0.4.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.5 has requirement python-dateutil>=2.7.3, but you'll have python-dateutil 2.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: googleapis-common-protos 1.53.0 has requirement protobuf>=3.12.0, but you'll have protobuf 3.9.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement astor~=0.8.1, but you'll have astor 0.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.2 has requirement protobuf>=3.12.0, but you'll have protobuf 3.9.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.2 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: flask 1.1.2 has requirement Jinja2>=2.10.1, but you'll have jinja2 2.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: flask 1.1.2 has requirement Werkzeug>=0.15, but you'll have werkzeug 0.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement python-dateutil>=2.8.0, but you'll have python-dateutil 2.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: bokeh 2.3.0 has requirement pillow>=7.1.0, but you'll have pillow 6.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTNOQgie2V1t",
        "outputId": "a96dbbed-04ea-4f49-f5bb-ffcf79f250e9"
      },
      "source": [
        "! ./download_pretrained.sh $CHOSEN_MODEL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading spanbert_base\n",
            "--2021-04-09 15:14:58--  http://nlp.cs.washington.edu/pair2vec/spanbert_base.tar.gz\n",
            "Resolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.120, 2607:4000:200:12::78\n",
            "Connecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.120|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1633726311 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘./spanbert_base.tar.gz’\n",
            "\n",
            "spanbert_base.tar.g 100%[===================>]   1.52G   107MB/s    in 15s     \n",
            "\n",
            "2021-04-09 15:15:14 (102 MB/s) - ‘./spanbert_base.tar.gz’ saved [1633726311/1633726311]\n",
            "\n",
            "spanbert_base/\n",
            "spanbert_base/checkpoint\n",
            "spanbert_base/model.max.ckpt.index\n",
            "spanbert_base/stdout.log\n",
            "spanbert_base/bert_config.json\n",
            "spanbert_base/vocab.txt\n",
            "spanbert_base/model.max.ckpt.data-00000-of-00001\n",
            "spanbert_base/events.out.tfevents.1561596094.learnfair1413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpNt3HqS4RwG",
        "outputId": "8c198f7e-0d3a-426a-a7f9-9476e43edbbe"
      },
      "source": [
        "from bert import tokenization\n",
        "import json\n",
        "\n",
        "data = {\n",
        "    'doc_key': genre,\n",
        "    'sentences': [[\"[CLS]\"]],\n",
        "    'speakers': [[\"[SPL]\"]],\n",
        "    'clusters': [],\n",
        "    'sentence_map': [0],\n",
        "    'subtoken_map': [0],\n",
        "}\n",
        "\n",
        "# Determine Max Segment\n",
        "max_segment = None\n",
        "for line in open('experiments.conf'):\n",
        "    if line.startswith(model_name):\n",
        "        max_segment = True\n",
        "    elif line.strip().startswith(\"max_segment_len\"):\n",
        "        if max_segment:\n",
        "            max_segment = int(line.strip().split()[-1])\n",
        "            break\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=\"cased_config_vocab/vocab.txt\", do_lower_case=False)\n",
        "subtoken_num = 0\n",
        "for sent_num, line in enumerate(text):\n",
        "    raw_tokens = line.split()\n",
        "    tokens = tokenizer.tokenize(line)\n",
        "    if len(tokens) + len(data['sentences'][-1]) >= max_segment:\n",
        "        data['sentences'][-1].append(\"[SEP]\")\n",
        "        data['sentences'].append([\"[CLS]\"])\n",
        "        data['speakers'][-1].append(\"[SPL]\")\n",
        "        data['speakers'].append([\"[SPL]\"])\n",
        "        data['sentence_map'].append(sent_num - 1)\n",
        "        data['subtoken_map'].append(subtoken_num - 1)\n",
        "        data['sentence_map'].append(sent_num)\n",
        "        data['subtoken_map'].append(subtoken_num)\n",
        "\n",
        "    ctoken = raw_tokens[0]\n",
        "    cpos = 0\n",
        "    for token in tokens:\n",
        "        data['sentences'][-1].append(token)\n",
        "        data['speakers'][-1].append(\"-\")\n",
        "        data['sentence_map'].append(sent_num)\n",
        "        data['subtoken_map'].append(subtoken_num)\n",
        "        \n",
        "        if token.startswith(\"##\"):\n",
        "            token = token[2:]\n",
        "        if len(ctoken) == len(token):\n",
        "            subtoken_num += 1\n",
        "            cpos += 1\n",
        "            if cpos < len(raw_tokens):\n",
        "                ctoken = raw_tokens[cpos]\n",
        "        else:\n",
        "            ctoken = ctoken[len(token):]\n",
        "\n",
        "data['sentences'][-1].append(\"[SEP]\")\n",
        "data['speakers'][-1].append(\"[SPL]\")\n",
        "data['sentence_map'].append(sent_num - 1)\n",
        "data['subtoken_map'].append(subtoken_num - 1)\n",
        "\n",
        "with open(\"sample.in.json\", 'w') as out:\n",
        "    json.dump(data, out, sort_keys=True)\n",
        "\n",
        "! cat sample.in.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0409 15:15:44.252221 139704741857152 deprecation_wrapper.py:119] From /content/coref/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"clusters\": [], \"doc_key\": \"mz\", \"sentence_map\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1], \"sentences\": [[\"[CLS]\", \"Victoria\", \"Chen\", \",\", \"CF\", \"##O\", \"of\", \"Mega\", \"##bu\", \"##cks\", \"Banking\", \",\", \"saw\", \"her\", \"pay\", \"jump\", \"to\", \"$\", \"2\", \".\", \"3\", \"million\", \",\", \"as\", \"the\", \"38\", \"-\", \"year\", \"-\", \"old\", \"became\", \"the\", \"company\", \"\\u2019\", \"s\", \"president\", \".\", \"It\", \"is\", \"widely\", \"known\", \"that\", \"she\", \"came\", \"to\", \"Mega\", \"##bu\", \"##cks\", \"from\", \"rival\", \"Lots\", \"##ab\", \"##ucks\", \".\", \"[SEP]\"]], \"speakers\": [[\"[SPL]\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"[SPL]\"]], \"subtoken_map\": [0, 0, 1, 1, 2, 2, 3, 4, 4, 4, 5, 5, 6, 7, 8, 9, 10, 11, 11, 11, 11, 12, 12, 13, 14, 15, 15, 15, 15, 15, 16, 17, 18, 18, 18, 19, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 28, 29, 30, 31, 31, 31, 31, 31]}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyk120c94uSW",
        "outputId": "f0bf7a28-4b94-444b-c4db-ea14422fb938"
      },
      "source": [
        "! GPU=0 python predict.py $CHOSEN_MODEL sample.in.json sample.out.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0409 15:15:51.812668 139794122307456 deprecation_wrapper.py:119] From /content/coref/coref_ops.py:11: The name tf.NotDifferentiable is deprecated. Please use tf.no_gradient instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n",
            "W0409 15:15:51.895436 139794122307456 deprecation_wrapper.py:119] From /content/coref/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0409 15:15:52.929668 139794122307456 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Setting CUDA_VISIBLE_DEVICES to: 0\n",
            "Running experiment: spanbert_base\n",
            "data_dir = \"/sdb/data/new_coref\"\n",
            "model_type = \"independent\"\n",
            "max_top_antecedents = 50\n",
            "max_training_sentences = 3\n",
            "top_span_ratio = 0.4\n",
            "max_num_speakers = 20\n",
            "max_segment_len = 384\n",
            "bert_learning_rate = 2e-05\n",
            "task_learning_rate = 0.0001\n",
            "num_docs = 2802\n",
            "dropout_rate = 0.3\n",
            "ffnn_size = 3000\n",
            "ffnn_depth = 1\n",
            "num_epochs = 20\n",
            "feature_size = 20\n",
            "max_span_width = 30\n",
            "use_metadata = true\n",
            "use_features = true\n",
            "use_segment_distance = true\n",
            "model_heads = true\n",
            "coref_depth = 2\n",
            "coarse_to_fine = true\n",
            "fine_grained = true\n",
            "use_prior = true\n",
            "train_path = \"./train.english.384.jsonlines\"\n",
            "eval_path = \"./dev.english.384.jsonlines\"\n",
            "conll_eval_path = \"./dev.english.v4_gold_conll\"\n",
            "single_example = true\n",
            "genres = [\n",
            "  \"bc\"\n",
            "  \"bn\"\n",
            "  \"mz\"\n",
            "  \"nw\"\n",
            "  \"pt\"\n",
            "  \"tc\"\n",
            "  \"wb\"\n",
            "]\n",
            "eval_frequency = 1000\n",
            "report_frequency = 100\n",
            "log_root = \".\"\n",
            "adam_eps = 1e-06\n",
            "task_optimizer = \"adam\"\n",
            "bert_config_file = \"./spanbert_base/bert_config.json\"\n",
            "vocab_file = \"./spanbert_base/vocab.txt\"\n",
            "tf_checkpoint = \"./spanbert_base/model.max.ckpt\"\n",
            "init_checkpoint = \"./spanbert_base/model.max.ckpt\"\n",
            "log_dir = \"./spanbert_base\"\n",
            "W0409 15:15:52.988015 139794122307456 deprecation_wrapper.py:119] From /content/coref/bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0409 15:15:53.057829 139794122307456 deprecation_wrapper.py:119] From /content/coref/independent.py:48: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0409 15:15:53.063302 139794122307456 deprecation_wrapper.py:119] From /content/coref/independent.py:50: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "W0409 15:15:53.066034 139794122307456 deprecation.py:323] From /content/coref/bert/modeling.py:158: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0409 15:15:53.077800 139794122307456 deprecation_wrapper.py:119] From /content/coref/bert/modeling.py:175: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0409 15:15:53.077951 139794122307456 deprecation_wrapper.py:119] From /content/coref/bert/modeling.py:175: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0409 15:15:53.138713 139794122307456 deprecation.py:506] From /content/coref/bert/modeling.py:362: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0409 15:15:53.174047 139794122307456 deprecation.py:323] From /content/coref/bert/modeling.py:676: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0409 15:15:55.517220 139794122307456 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0409 15:15:55.583050 139794122307456 deprecation.py:323] From /content/coref/independent.py:221: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0409 15:15:55.623624 139794122307456 deprecation_wrapper.py:119] From /content/coref/util.py:117: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
            "\n",
            "W0409 15:15:56.103380 139794122307456 deprecation_wrapper.py:119] From /content/coref/independent.py:60: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "**** Trainable Variables ****\n",
            "  name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = span_width_embeddings:0, shape = (30, 20), *INIT_FROM_CKPT*\n",
            "  name = mention_word_attn/output_weights:0, shape = (768, 1), *INIT_FROM_CKPT*\n",
            "  name = mention_word_attn/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/hidden_weights_0:0, shape = (2324, 3000), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/hidden_bias_0:0, shape = (3000,), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/output_weights:0, shape = (3000, 1), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = span_width_prior_embeddings:0, shape = (30, 20), *INIT_FROM_CKPT*\n",
            "  name = width_scores/hidden_weights_0:0, shape = (20, 3000), *INIT_FROM_CKPT*\n",
            "  name = width_scores/hidden_bias_0:0, shape = (3000,), *INIT_FROM_CKPT*\n",
            "  name = width_scores/output_weights:0, shape = (3000, 1), *INIT_FROM_CKPT*\n",
            "  name = width_scores/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = genre_embeddings:0, shape = (7, 20), *INIT_FROM_CKPT*\n",
            "  name = src_projection/output_weights:0, shape = (2324, 2324), *INIT_FROM_CKPT*\n",
            "  name = src_projection/output_bias:0, shape = (2324,), *INIT_FROM_CKPT*\n",
            "  name = antecedent_distance_emb:0, shape = (10, 20), *INIT_FROM_CKPT*\n",
            "  name = output_weights:0, shape = (20, 1), *INIT_FROM_CKPT*\n",
            "  name = output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/same_speaker_emb:0, shape = (2, 20), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/antecedent_distance_emb:0, shape = (10, 20), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/segment_distance/segment_distance_embeddings:0, shape = (3, 20), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/hidden_weights_0:0, shape = (7052, 3000), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/hidden_bias_0:0, shape = (3000,), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/output_weights:0, shape = (3000, 1), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/f/output_weights:0, shape = (4648, 2324), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/f/output_bias:0, shape = (2324,), *INIT_FROM_CKPT*\n",
            "W0409 15:15:56.835753 139794122307456 deprecation_wrapper.py:119] From /content/coref/independent.py:74: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0409 15:15:56.840731 139794122307456 deprecation_wrapper.py:119] From /content/coref/optimization.py:13: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0409 15:15:56.844080 139794122307456 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0409 15:15:56.861106 139794122307456 deprecation_wrapper.py:119] From /content/coref/optimization.py:64: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "bert:task 199 27\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "2021-04-09 15:16:05.613911: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-09 15:16:05.665647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-09 15:16:05.666260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-09 15:16:05.666332: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-04-09 15:16:05.859539: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2021-04-09 15:16:05.960076: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2021-04-09 15:16:05.978474: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2021-04-09 15:16:06.171739: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2021-04-09 15:16:06.290430: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2021-04-09 15:16:06.643012: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-09 15:16:06.643192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-09 15:16:06.643864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-09 15:16:06.644399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2021-04-09 15:16:06.644762: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-04-09 15:16:06.649313: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-04-09 15:16:06.649541: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562350141800 executing computations on platform Host. Devices:\n",
            "2021-04-09 15:16:06.649570: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2021-04-09 15:16:06.765371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-09 15:16:06.766644: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5623565be8c0 executing computations on platform CUDA. Devices:\n",
            "2021-04-09 15:16:06.766689: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2021-04-09 15:16:06.766944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-09 15:16:06.767741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-09 15:16:06.767832: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-04-09 15:16:06.767896: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2021-04-09 15:16:06.767921: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2021-04-09 15:16:06.767940: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2021-04-09 15:16:06.767960: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2021-04-09 15:16:06.767975: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2021-04-09 15:16:06.767989: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-09 15:16:06.768085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-09 15:16:06.768744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-09 15:16:06.769522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2021-04-09 15:16:06.769594: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-04-09 15:16:09.368831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-09 15:16:09.368907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2021-04-09 15:16:09.368921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2021-04-09 15:16:09.369164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-09 15:16:09.369781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-09 15:16:09.370326: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-09 15:16:09.370370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14747 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "Restoring from ./spanbert_base/model.max.ckpt\n",
            "W0409 15:16:17.796588 139794122307456 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2021-04-09 15:16:20.346999: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "Decoded 1 examples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3Z0kTLO4yzl",
        "outputId": "7d1d51a7-3a6d-4375-de9a-3f985d9c9e78"
      },
      "source": [
        "output = json.load(open(\"sample.out.txt\"))\n",
        "\n",
        "comb_text = [word for sentence in output['sentences'] for word in sentence]\n",
        "\n",
        "def convert_mention(mention):\n",
        "    start = output['subtoken_map'][mention[0]]\n",
        "    end = output['subtoken_map'][mention[1]] + 1\n",
        "    nmention = (start, end)\n",
        "    mtext = ''.join(' '.join(comb_text[mention[0]:mention[1]+1]).split(\" ##\"))\n",
        "    return (nmention, mtext)\n",
        "\n",
        "seen = set()\n",
        "print('Clusters:')\n",
        "for cluster in output['predicted_clusters']:\n",
        "    mapped = []\n",
        "    for mention in cluster:\n",
        "        seen.add(tuple(mention))\n",
        "        mapped.append(convert_mention(mention))\n",
        "    print(mapped, end=\",\\n\")\n",
        "\n",
        "print('\\nMentions:')\n",
        "for mention in output['top_spans']:\n",
        "    if tuple(mention) in seen:\n",
        "        continue\n",
        "    print(convert_mention(mention), end=\",\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clusters:\n",
            "[((0, 6), 'Victoria Chen , CFO of Megabucks Banking'), ((7, 8), 'her'), ((14, 16), 'the 38 - year - old'), ((25, 26), 'she')],\n",
            "[((4, 6), 'Megabucks Banking'), ((17, 19), 'the company ’ s'), ((28, 29), 'Megabucks')],\n",
            "\n",
            "Mentions:\n",
            "((0, 1), '[CLS]'),\n",
            "((0, 6), '[CLS] Victoria Chen , CFO of Megabucks Banking ,'),\n",
            "((0, 6), 'Victoria Chen , CFO of Megabucks Banking ,'),\n",
            "((2, 6), 'CFO of Megabucks Banking'),\n",
            "((11, 13), '$ 2 . 3 million'),\n",
            "((14, 32), 'the 38 - year - old became the company ’ s president . It is widely known that she came to Megabucks from rival Lotsabucks'),\n",
            "((17, 20), 'the company ’ s president'),\n",
            "((17, 29), 'the company ’ s president . It is widely known that she came to Megabucks'),\n",
            "((17, 32), 'the company ’ s president . It is widely known that she came to Megabucks from rival Lotsabucks'),\n",
            "((20, 21), 'It'),\n",
            "((20, 27), 'It is widely known that she came'),\n",
            "((26, 27), 'came'),\n",
            "((30, 32), 'rival Lotsabucks'),\n",
            "((31, 32), '.'),\n",
            "((31, 32), '[SEP]'),\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46hfkGod5LEx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_OMoqxj8tq_"
      },
      "source": [
        "## DE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxjdlaRH8x0C",
        "outputId": "b49da6cc-f02d-4cda-f19b-9d4b429a2f3d"
      },
      "source": [
        "from bert import tokenization\n",
        "import json\n",
        "\n",
        "data = {\n",
        "    'doc_key': genre,\n",
        "    'sentences': [[\"[CLS]\"]],\n",
        "    'speakers': [[\"[SPL]\"]],\n",
        "    'clusters': [],\n",
        "    'sentence_map': [0],\n",
        "    'subtoken_map': [0],\n",
        "}\n",
        "\n",
        "# Determine Max Segment\n",
        "max_segment = None\n",
        "for line in open('experiments.conf'):\n",
        "    if line.startswith(model_name):\n",
        "        max_segment = True\n",
        "    elif line.strip().startswith(\"max_segment_len\"):\n",
        "        if max_segment:\n",
        "            max_segment = int(line.strip().split()[-1])\n",
        "            break\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=\"cased_config_vocab/vocab.txt\", do_lower_case=False)\n",
        "subtoken_num = 0\n",
        "for sent_num, line in enumerate(text):\n",
        "    raw_tokens = line.split()\n",
        "    tokens = tokenizer.tokenize(line)\n",
        "    if len(tokens) + len(data['sentences'][-1]) >= max_segment:\n",
        "        data['sentences'][-1].append(\"[SEP]\")\n",
        "        data['sentences'].append([\"[CLS]\"])\n",
        "        data['speakers'][-1].append(\"[SPL]\")\n",
        "        data['speakers'].append([\"[SPL]\"])\n",
        "        data['sentence_map'].append(sent_num - 1)\n",
        "        data['subtoken_map'].append(subtoken_num - 1)\n",
        "        data['sentence_map'].append(sent_num)\n",
        "        data['subtoken_map'].append(subtoken_num)\n",
        "\n",
        "    ctoken = raw_tokens[0]\n",
        "    cpos = 0\n",
        "    for token in tokens:\n",
        "        data['sentences'][-1].append(token)\n",
        "        data['speakers'][-1].append(\"-\")\n",
        "        data['sentence_map'].append(sent_num)\n",
        "        data['subtoken_map'].append(subtoken_num)\n",
        "        \n",
        "        if token.startswith(\"##\"):\n",
        "            token = token[2:]\n",
        "        if len(ctoken) == len(token):\n",
        "            subtoken_num += 1\n",
        "            cpos += 1\n",
        "            if cpos < len(raw_tokens):\n",
        "                ctoken = raw_tokens[cpos]\n",
        "        else:\n",
        "            ctoken = ctoken[len(token):]\n",
        "\n",
        "data['sentences'][-1].append(\"[SEP]\")\n",
        "data['speakers'][-1].append(\"[SPL]\")\n",
        "data['sentence_map'].append(sent_num - 1)\n",
        "data['subtoken_map'].append(subtoken_num - 1)\n",
        "\n",
        "with open(\"sample.in.json\", 'w') as out:\n",
        "    json.dump(data, out, sort_keys=True)\n",
        "\n",
        "! cat sample.in.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"clusters\": [], \"doc_key\": \"nw\", \"sentence_map\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15], \"sentences\": [[\"[CLS]\", \"Das\", \"\\u00a32\", \"-\", \"pro\", \"Tag\", \"-\", \"Me\", \"##di\", \"##ka\", \"##ment\", \",\", \"das\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"##ris\", \"##iko\", \"se\", \"##nk\", \"##en\", \"ka\", \"##nn\", \"Tau\", \"##sen\", \"##de\", \"von\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"##patient\", \"##en\", \"we\", \"##rden\", \"von\", \"e\", \"##ine\", \"##m\", \"\\u00a32\", \"-\", \"pro\", \"Tag\", \"-\", \"Me\", \"##di\", \"##ka\", \"##ment\", \"profit\", \"##ier\", \"##en\", \",\", \"na\", \"##ch\", \"##de\", \"##m\", \"von\", \"den\", \"Be\", \"##h\", \"##\\u00f6\", \"##rden\", \"em\", \"##pf\", \"##oh\", \"##len\", \"w\", \"##ur\", \"##de\", \",\", \"dies\", \"##es\", \"me\", \"##hr\", \"Le\", \"##ute\", \"##n\", \"f\\u00fcr\", \"e\", \"##inen\", \"l\", \"##\\u00e4\", \"##nger\", \"##en\", \"Z\", \"##eit\", \"##ra\", \"##um\", \"zu\", \"ve\", \"##rab\", \"##re\", \"##iche\", \"##n\", \".\", \"Das\", \"g\", \"##eri\", \"##nn\", \"##ung\", \"##she\", \"##mme\", \"##nde\", \"Me\", \"##di\", \"##ka\", \"##ment\", \"T\", \"##ica\", \"##g\", \"##rel\", \"##or\", \"red\", \"##uz\", \"##ier\", \"##t\", \"das\", \"R\", \"##isi\", \"##ko\", \"w\", \"##ied\", \"##er\", \"##hol\", \"##ter\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"##e\", \"f\\u00fcr\", \"Person\", \"##en\", \"mit\", \"e\", \"##iner\", \"Her\", \"##zer\", \"##kra\", \"##nk\", \"##ung\", \".\", \"Das\", \"Me\", \"##di\", \"##ka\", \"##ment\", \"w\", \"##ir\", \"##d\", \"be\", \"##re\", \"##its\", \"12\", \"Mona\", \"##te\", \"la\", \"##ng\", \"na\", \"##ch\", \"e\", \"##ine\", \"##m\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"ve\", \"##rab\", \"##re\", \"##ich\", \"##t\", \",\", \"w\", \"##od\", \"##ur\", \"##ch\", \"das\", \"R\", \"##isi\", \"##ko\", \"e\", \"##ines\", \"Sc\", \"##hl\", \"##agan\", \"##falls\", \"o\", \"##der\", \"e\", \"##ines\", \"we\", \"##iter\", \"##en\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"##s\", \"red\", \"##uz\", \"##ier\", \"##t\", \"w\", \"##ir\", \"##d\", \".\", \"Die\", \"Reg\", \"##uli\", \"##er\", \"##ung\", \"##s\", \"##be\", \"##h\", \"##\\u00f6\", \"##rde\", \"N\", \"##IC\", \"##E\", \"des\", \"NHS\", \"hat\", \"e\", \"##ine\", \"v\", \"##ier\", \"J\", \"##ah\", \"##re\", \"la\", \"##nge\", \"Ein\", \"##nah\", \"##me\", \"em\", \"##pf\", \"##oh\", \"##len\", \",\", \"um\", \"das\", \"R\", \"##isi\", \"##ko\", \"ka\", \"##rdi\", \"##ova\", \"##sk\", \"##ul\", \"##\\u00e4\", \"##rer\", \"Problem\", \"##e\", \"we\", \"##iter\", \"zu\", \"red\", \"##uz\", \"##ier\", \"##en\", \".\", \"Die\", \"Reg\", \"##uli\", \"##er\", \"##ung\", \"##s\", \"##be\", \"##h\", \"##\\u00f6\", \"##rde\", \"N\", \"##IC\", \"##E\", \"des\", \"NHS\", \"hat\", \"e\", \"##ine\", \"v\", \"##ier\", \"J\", \"##ah\", \"##re\", \"la\", \"##nge\", \"Ein\", \"##nah\", \"##me\", \"em\", \"##pf\", \"##oh\", \"##len\", \",\", \"um\", \"das\", \"R\", \"##isi\", \"##ko\", \"ka\", \"##rdi\", \"##ova\", \"##sk\", \"##ul\", \"##\\u00e4\", \"##rer\", \"Problem\", \"##e\", \"we\", \"##iter\", \"zu\", \"red\", \"##uz\", \"##ier\", \"##en\", \".\", \"Run\", \"##d\", \"140\", \".\", \"000\", \"Le\", \"##ute\", \"er\", \"##le\", \"##iden\", \"j\", \"##ede\", \"##s\", \"J\", \"##ah\", \"##r\", \"e\", \"##inen\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"und\", \"e\", \"##in\", \"V\", \"##ier\", \"##tel\", \"da\", \"##von\", \"er\", \"##le\", \"##iden\", \"e\", \"##inen\", \"we\", \"##iter\", \"##en\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"o\", \"##der\", \"e\", \"##inen\", \"Sc\", \"##hl\", \"##agan\", \"##fall\", \".\", \"[SEP]\"], [\"[CLS]\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"##e\", \"und\", \"Sc\", \"##hl\", \"##agan\", \"##f\", \"##\\u00e4\", \"##lle\", \"we\", \"##rden\", \"du\", \"##rch\", \"An\", \"##sa\", \"##mm\", \"##lung\", \"##en\", \"f\", \"##ett\", \"##halt\", \"##ige\", \"##n\", \"Materials\", \"in\", \"den\", \"Arte\", \"##rien\", \"##w\", \"##\\u00e4\", \"##nden\", \"ve\", \"##ru\", \"##rsa\", \"##cht\", \",\", \"die\", \"Bel\", \"##\\u00e4\", \"##ge\", \"bi\", \"##lde\", \"##n\", \".\", \"Wen\", \"##n\", \"der\", \"Bel\", \"##ag\", \"au\", \"##sei\", \"##nan\", \"##der\", \"##bri\", \"##cht\", \",\", \"ka\", \"##nn\", \"es\", \"e\", \"##in\", \"Blu\", \"##t\", \"##ger\", \"##inn\", \"##sel\", \"ve\", \"##ru\", \"##rsa\", \"##chen\", \",\", \"das\", \"den\", \"Blu\", \"##t\", \"##f\", \"##lus\", \"##s\", \"bi\", \"##s\", \"zu\", \"##m\", \"Her\", \"##zen\", \"ve\", \"##rst\", \"##op\", \"##ft\", \"und\", \"dad\", \"##ur\", \"##ch\", \"e\", \"##inen\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"ve\", \"##ru\", \"##rsa\", \"##cht\", \".\", \"Wen\", \"##n\", \"sic\", \"##h\", \"das\", \"Blu\", \"##t\", \"##ger\", \"##inn\", \"##sel\", \"l\", \"##\\u00f6\", \"##st\", \",\", \"ka\", \"##nn\", \"es\", \"du\", \"##rch\", \"den\", \"Blu\", \"##ts\", \"##trom\", \"fl\", \"##ie\", \"##\\u00dfe\", \"##n\", \"und\", \"den\", \"Blu\", \"##t\", \"##f\", \"##lus\", \"##s\", \"zu\", \"##m\", \"G\", \"##eh\", \"##irn\", \"ve\", \"##rst\", \"##op\", \"##fen\", \",\", \"was\", \"e\", \"##inen\", \"Sc\", \"##hl\", \"##agan\", \"##fall\", \"ve\", \"##ru\", \"##rsa\", \"##cht\", \".\", \"Person\", \"##en\", \",\", \"die\", \"be\", \"##re\", \"##its\", \"e\", \"##inen\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"hat\", \"##ten\", \",\", \"un\", \"##ter\", \"##lie\", \"##gen\", \"e\", \"##ine\", \"##m\", \"h\", \"##\\u00f6\", \"##here\", \"##n\", \"R\", \"##isi\", \"##ko\", \"e\", \"##inen\", \"we\", \"##iter\", \"##en\", \"zu\", \"ha\", \"##ben\", \".\", \"T\", \"##ica\", \"##g\", \"##rel\", \"##or\", \",\", \"das\", \"von\", \"dem\", \"br\", \"##itis\", \"##chen\", \"Un\", \"##tern\", \"##eh\", \"##men\", \"As\", \"##tra\", \"##Z\", \"##ene\", \"##ca\", \"her\", \"##ges\", \"##tel\", \"##lt\", \"w\", \"##ir\", \"##d\", \"und\", \"un\", \"##ter\", \"dem\", \"Handel\", \"##s\", \"##name\", \"##n\", \"B\", \"##ril\", \"##ique\", \"ve\", \"##rt\", \"##rie\", \"##ben\", \"w\", \"##ir\", \"##d\", \",\", \"red\", \"##uz\", \"##ier\", \"##t\", \"dies\", \"##es\", \"R\", \"##isi\", \"##ko\", \",\", \"in\", \"##de\", \"##m\", \"es\", \"die\", \"B\", \"##il\", \"##du\", \"##ng\", \"von\", \"Blu\", \"##t\", \"##ger\", \"##inn\", \"##sel\", \"##n\", \"un\", \"##wa\", \"##hr\", \"##sche\", \"##in\", \"##liche\", \"##r\", \"mac\", \"##ht\", \".\", \"Der\", \"An\", \"##le\", \"##it\", \"##ung\", \"##sent\", \"##wu\", \"##rf\", \"von\", \"N\", \"##IC\", \"##E\", \",\", \"der\", \"he\", \"##ute\", \"ve\", \"##r\", \"##\\u00f6\", \"##ffe\", \"##nt\", \"##lich\", \"##t\", \"w\", \"##ur\", \"##de\", \",\", \"em\", \"##pf\", \"##ie\", \"##hl\", \"##t\", \"e\", \"##ine\", \"12\", \"-\", \"mon\", \"##ati\", \"##ge\", \"Ein\", \"##nah\", \"##me\", \"von\", \"90\", \"##m\", \"##g\", \"T\", \"##ica\", \"##g\", \"##rel\", \"##or\", \",\", \"g\", \"##ef\", \"##ol\", \"##gt\", \"von\", \"60\", \"##m\", \"##g\", \"mit\", \"e\", \"##iner\", \"z\", \"##wei\", \"##mal\", \"t\", \"##\\u00e4\", \"##gli\", \"##chen\", \"Ein\", \"##nah\", \"##me\", \"von\", \"As\", \"##pi\", \"##rin\", \"f\\u00fcr\", \"die\", \"n\", \"##\\u00e4\", \"##chs\", \"##ten\", \"d\", \"##re\", \"##i\", \"J\", \"##ah\", \"##re\", \".\", \"[SEP]\"], [\"[CLS]\", \"Professor\", \"Carole\", \"Long\", \"##son\", \",\", \"Di\", \"##rek\", \"##tor\", \"##in\", \"des\", \"N\", \"##IC\", \"##E\", \"G\", \"##es\", \"##und\", \"##he\", \"##its\", \"##tech\", \"##no\", \"##log\", \"##ie\", \"##eval\", \"##ui\", \"##er\", \"##ung\", \"##sz\", \"##ent\", \"##rum\", \"##s\", \"sa\", \"##gt\", \"##e\", \":\", \"\\\"\", \"T\", \"##rot\", \"##z\", \"der\", \"V\", \"##er\", \"##f\", \"##\\u00fc\", \"##g\", \"##bark\", \"##eit\", \"von\", \"Se\", \"##ku\", \"##nd\", \"##\\u00e4\", \"##rp\", \"##r\", \"##\\u00e4\", \"##vention\", \"ha\", \"##ben\", \"e\", \"##in\", \"V\", \"##ier\", \"##tel\", \"all\", \"##er\", \"Person\", \"##en\", \",\", \"die\", \"e\", \"##inen\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"er\", \"##lit\", \"##ten\", \"ha\", \"##ben\", \",\", \"e\", \"##inen\", \"we\", \"##iter\", \"##en\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"o\", \"##der\", \"e\", \"##inen\", \"Sc\", \"##hl\", \"##agan\", \"##fall\", \"-\", \"of\", \"##t\", \"mit\", \"des\", \"##ast\", \"##r\", \"##\\u00f6\", \"##sen\", \"F\", \"##ol\", \"##gen\", \".\", \"Die\", \"Ang\", \"##st\", \"v\", \"##or\", \"e\", \"##ine\", \"##m\", \"er\", \"##ne\", \"##ute\", \"##n\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"ka\", \"##nn\", \"er\", \"##he\", \"##b\", \"##liche\", \"negative\", \"Au\", \"##s\", \"##wi\", \"##rk\", \"##ungen\", \"au\", \"##f\", \"die\", \"Le\", \"##ben\", \"##s\", \"##qua\", \"##lit\", \"##\\u00e4t\", \"e\", \"##iner\", \"Person\", \"ha\", \"##ben\", \".\", \"Die\", \"E\", \"##rf\", \"##ah\", \"##rung\", \"z\", \"##ei\", \"##gt\", \",\", \"das\", \"##s\", \"T\", \"##ica\", \"##g\", \"##rel\", \"##or\", \"in\", \"Ko\", \"##mb\", \"##ination\", \"mit\", \"As\", \"##pi\", \"##rin\", \"e\", \"##ffe\", \"##kt\", \"##iv\", \"be\", \"##i\", \"der\", \"Red\", \"##uz\", \"##ier\", \"##ung\", \"we\", \"##iter\", \"##er\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"##e\", \"und\", \"Sc\", \"##hl\", \"##agan\", \"##f\", \"##\\u00e4\", \"##lle\", \"be\", \"##i\", \"Le\", \"##ute\", \"##n\", \"is\", \"##t\", \",\", \"die\", \"be\", \"##re\", \"##its\", \"e\", \"##inen\", \"Her\", \"##zin\", \"##far\", \"##kt\", \"hat\", \"##ten\", \".\", \"Du\", \"##rch\", \"e\", \"##ine\", \"v\", \"##or\", \"##l\", \"##\\u00e4\", \"##uf\", \"##ige\", \"Em\", \"##pf\", \"##eh\", \"##lung\", \"von\", \"T\", \"##ica\", \"##g\", \"##rel\", \"##or\", \"f\", \"##re\", \"##uen\", \"w\", \"##ir\", \"un\", \"##s\", \",\", \"das\", \"##s\", \"w\", \"##ir\", \"in\", \"der\", \"La\", \"##ge\", \"sin\", \"##d\", \",\", \"ve\", \"##rf\", \"##\\u00fc\", \"##g\", \"##bar\", \"##e\", \"Be\", \"##hand\", \"##lung\", \"##so\", \"##ption\", \"##en\", \"an\", \"ta\", \"##use\", \"##nde\", \"von\", \"Men\", \"##schen\", \"zu\", \"er\", \"##wei\", \"##tern\", \",\", \"die\", \"da\", \"##von\", \"profit\", \"##ier\", \"##en\", \"k\", \"##\\u00f6\", \"##nne\", \"##n\", \".\", \"[SEP]\"], [\"[CLS]\", \"Die\", \"Information\", \"\\u00fc\", \"##ber\", \"die\", \"W\", \"##irk\", \"##sa\", \"##m\", \"##ke\", \"##it\", \"und\", \"Si\", \"##cher\", \"##he\", \"##it\", \"von\", \"T\", \"##ica\", \"##g\", \"##rel\", \"##or\", \"-\", \"v\", \"##or\", \"all\", \"##em\", \"das\", \"Blu\", \"##tun\", \"##gs\", \"##ris\", \"##iko\", \"-\", \"is\", \"##t\", \"au\", \"##f\", \"e\", \"##inen\", \"Z\", \"##eit\", \"##ra\", \"##um\", \"von\", \"bi\", \"##s\", \"zu\", \"d\", \"##re\", \"##i\", \"J\", \"##ah\", \"##ren\", \"be\", \"##sch\", \"##r\", \"##\\u00e4\", \"##nk\", \"##t\", \".\", \"Der\", \"An\", \"##le\", \"##it\", \"##ung\", \"##sent\", \"##wu\", \"##rf\", \"em\", \"##pf\", \"##ie\", \"##hl\", \"##t\", \"k\", \"##ein\", \"##e\", \"Be\", \"##hand\", \"##lung\", \",\", \"die\", \"\\u00fc\", \"##ber\", \"dies\", \"##en\", \"Z\", \"##eit\", \"##ra\", \"##um\", \"hi\", \"##na\", \"##us\", \"##ge\", \"##ht\", \".\", \"[SEP]\"]], \"speakers\": [[\"[SPL]\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"[SPL]\"], [\"[SPL]\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"[SPL]\"], [\"[SPL]\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"[SPL]\"], [\"[SPL]\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"[SPL]\"]], \"subtoken_map\": [0, 0, 1, 1, 1, 2, 2, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8, 8, 8, 9, 10, 10, 10, 10, 10, 10, 11, 11, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 18, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 22, 23, 23, 24, 24, 25, 25, 25, 26, 27, 27, 28, 28, 28, 28, 29, 29, 29, 29, 30, 31, 31, 31, 31, 31, 31, 32, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 35, 35, 35, 35, 35, 36, 36, 36, 36, 37, 38, 38, 38, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 41, 42, 42, 43, 44, 44, 45, 45, 45, 45, 45, 45, 46, 47, 47, 47, 47, 48, 48, 48, 49, 49, 49, 50, 51, 51, 52, 52, 53, 53, 54, 54, 54, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 58, 59, 59, 59, 60, 60, 61, 61, 61, 61, 62, 62, 63, 63, 64, 64, 64, 65, 65, 65, 65, 65, 66, 66, 66, 66, 67, 67, 67, 67, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 71, 72, 73, 74, 74, 75, 75, 76, 76, 76, 77, 77, 78, 78, 78, 79, 79, 79, 79, 79, 80, 81, 82, 82, 82, 83, 83, 83, 83, 83, 83, 83, 84, 84, 85, 85, 86, 87, 87, 87, 87, 87, 88, 89, 89, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 91, 92, 93, 94, 94, 95, 95, 96, 96, 96, 97, 97, 98, 98, 98, 99, 99, 99, 99, 99, 100, 101, 102, 102, 102, 103, 103, 103, 103, 103, 103, 103, 104, 104, 105, 105, 106, 107, 107, 107, 107, 107, 108, 108, 109, 109, 109, 110, 110, 111, 111, 111, 112, 112, 112, 113, 113, 113, 114, 114, 115, 115, 115, 115, 116, 117, 117, 118, 118, 118, 119, 119, 120, 120, 120, 121, 121, 122, 122, 122, 123, 123, 123, 123, 124, 124, 125, 125, 126, 126, 126, 126, 126, 126, 127, 127, 127, 127, 127, 127, 128, 129, 129, 129, 129, 129, 129, 130, 130, 131, 131, 132, 132, 132, 132, 132, 133, 133, 133, 133, 133, 134, 135, 136, 137, 137, 137, 137, 137, 138, 138, 138, 138, 138, 139, 140, 140, 140, 141, 141, 141, 141, 142, 142, 143, 144, 144, 145, 145, 145, 145, 145, 145, 145, 146, 146, 147, 148, 148, 149, 149, 149, 149, 149, 150, 150, 150, 150, 150, 151, 152, 153, 153, 153, 153, 153, 154, 154, 155, 155, 156, 156, 157, 157, 157, 157, 158, 159, 159, 159, 160, 160, 161, 161, 161, 161, 162, 162, 162, 162, 162, 163, 163, 164, 164, 165, 166, 166, 166, 166, 166, 167, 167, 167, 167, 168, 168, 169, 170, 170, 171, 172, 172, 172, 173, 173, 173, 173, 174, 175, 176, 176, 176, 176, 176, 177, 177, 178, 178, 178, 179, 179, 179, 179, 179, 180, 181, 181, 182, 182, 182, 182, 183, 183, 183, 183, 183, 184, 184, 184, 185, 186, 186, 186, 187, 187, 188, 188, 188, 188, 189, 189, 189, 190, 190, 190, 190, 191, 191, 191, 192, 192, 192, 192, 193, 193, 193, 194, 194, 195, 195, 195, 196, 197, 197, 197, 198, 198, 198, 198, 198, 198, 199, 200, 201, 202, 202, 202, 203, 203, 203, 203, 204, 204, 204, 204, 204, 205, 205, 205, 205, 206, 206, 206, 207, 208, 208, 209, 210, 210, 210, 210, 211, 211, 211, 212, 212, 212, 212, 213, 213, 213, 213, 214, 214, 214, 214, 215, 215, 216, 216, 216, 216, 217, 217, 217, 218, 219, 220, 220, 220, 220, 221, 222, 222, 222, 222, 222, 222, 223, 223, 223, 223, 223, 223, 223, 224, 224, 224, 225, 226, 226, 226, 226, 226, 226, 226, 227, 228, 228, 228, 228, 229, 230, 230, 231, 231, 231, 231, 231, 231, 231, 232, 232, 232, 232, 233, 233, 233, 233, 233, 234, 234, 235, 235, 235, 235, 235, 236, 236, 236, 237, 238, 238, 238, 239, 239, 239, 239, 239, 239, 240, 240, 240, 240, 241, 242, 242, 242, 243, 244, 244, 245, 245, 245, 246, 246, 246, 246, 247, 247, 247, 248, 249, 249, 249, 250, 251, 252, 252, 252, 252, 253, 253, 253, 254, 254, 254, 254, 254, 255, 255, 256, 257, 257, 257, 258, 258, 258, 258, 259, 260, 260, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 262, 262, 262, 262, 263, 263, 263, 263, 264, 265, 265, 265, 265, 265, 265, 265, 266, 267, 267, 267, 267, 267, 267, 267, 267, 268, 268, 269, 269, 270, 270, 270, 271, 271, 272, 272, 272, 273, 274, 274, 275, 275, 275, 275, 276, 276, 276, 277, 277, 277, 278, 278, 279, 279, 279, 280, 280, 280, 280, 281, 281, 282, 282, 283, 283, 283, 283, 284, 285, 285, 286, 287, 287, 287, 287, 287, 288, 288, 288, 288, 288, 289, 289, 290, 290, 291, 291, 291, 292, 292, 292, 292, 293, 293, 293, 293, 294, 294, 295, 295, 295, 295, 296, 297, 297, 297, 297, 297, 298, 298, 299, 300, 300, 300, 300, 300, 300, 301, 301, 302, 303, 303, 303, 304, 305, 305, 305, 305, 306, 306, 306, 306, 307, 307, 308, 308, 308, 308, 308, 309, 310, 310, 310, 311, 312, 312, 312, 313, 313, 313, 313, 314, 314, 315, 316, 316, 316, 316, 317, 317, 317, 318, 318, 318, 318, 318, 319, 320, 320, 320, 320, 320, 320, 321, 321, 322, 322, 322, 323, 323, 323, 324, 325, 325, 325, 326, 326, 327, 327, 327, 327, 328, 328, 328, 329, 329, 330, 330, 331, 331, 331, 331, 331, 331, 332, 332, 332, 332, 333, 334, 334, 334, 334, 334, 335, 335, 335, 336, 336, 337, 337, 337, 338, 338, 339, 339, 340, 341, 342, 342, 343, 343, 343, 344, 344, 344, 344, 344, 344, 345, 345, 345, 345, 345, 345, 346, 347, 347, 347, 348, 349, 349, 350, 351, 351, 351, 351, 352, 353, 353, 354, 354, 354, 355, 355, 355, 355, 355, 355, 356, 356, 357, 358, 358, 359, 360, 360, 360, 360, 360, 360, 361, 362, 362, 362, 362, 363, 364, 364, 364, 364, 364, 365, 366, 366, 367, 367, 368, 369, 369, 369, 369, 369, 370, 371, 371, 372, 372, 373, 373, 374, 374, 374, 374, 375, 376, 376, 377, 378, 378, 378, 379, 379, 379, 380, 380, 380, 380, 380, 380, 380, 381, 382, 382, 382, 382, 382, 382, 382, 383, 383, 383, 383, 383, 384, 384, 384, 385, 385, 385, 385, 386, 387, 387, 388, 388, 389, 389, 389, 389, 390, 390, 390, 390, 390, 390, 390]}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QNj7zQk8y8r",
        "outputId": "6ab6ee63-48c9-4cce-c42e-4ff3486c1538"
      },
      "source": [
        "output = json.load(open(\"sample.out.txt\"))\n",
        "\n",
        "comb_text = [word for sentence in output['sentences'] for word in sentence]\n",
        "\n",
        "def convert_mention(mention):\n",
        "    start = output['subtoken_map'][mention[0]]\n",
        "    end = output['subtoken_map'][mention[1]] + 1\n",
        "    nmention = (start, end)\n",
        "    mtext = ''.join(' '.join(comb_text[mention[0]:mention[1]+1]).split(\" ##\"))\n",
        "    return (nmention, mtext)\n",
        "\n",
        "seen = set()\n",
        "print('Clusters:')\n",
        "for cluster in output['predicted_clusters']:\n",
        "    mapped = []\n",
        "    for mention in cluster:\n",
        "        seen.add(tuple(mention))\n",
        "        mapped.append(convert_mention(mention))\n",
        "    print(mapped, end=\",\\n\")\n",
        "\n",
        "print('\\nMentions:')\n",
        "for mention in output['top_spans']:\n",
        "    if tuple(mention) in seen:\n",
        "        continue\n",
        "    print(convert_mention(mention), end=\",\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clusters:\n",
            "[((198, 199), 'Ticagrelor'), ((239, 240), 'Ticagrelor'), ((364, 365), 'Ticagrelor')],\n",
            "\n",
            "Mentions:\n",
            "((0, 1), '[CLS]'),\n",
            "((0, 4), 'Das £2 - pro Tag - Medikament'),\n",
            "((1, 2), '£2 - pro'),\n",
            "((2, 4), 'Tag - Medikament'),\n",
            "((6, 7), 'senken'),\n",
            "((6, 8), 'senken kann'),\n",
            "((8, 10), 'Tausende von'),\n",
            "((8, 17), 'Tausende von Herzinfarktpatienten werden von einem £2 - pro Tag - Medikament profitieren'),\n",
            "((8, 17), 'Tausende von Herzinfarktpatienten werden von einem £2 - pro Tag - Medikament profitieren ,'),\n",
            "((10, 11), 'Her'),\n",
            "((10, 12), 'Herzinfarktpatienten werden'),\n",
            "((10, 13), 'Herzinfarktpatienten werden von'),\n",
            "((10, 17), 'Herzinfarktpatienten werden von einem £2 - pro Tag - Medikament profitieren'),\n",
            "((11, 12), 'we'),\n",
            "((11, 12), 'werden'),\n",
            "((11, 12), '##rden'),\n",
            "((12, 13), 'von'),\n",
            "((13, 16), 'einem £2 - pro Tag - Medikament'),\n",
            "((13, 17), 'einem £2 - pro Tag - Medikament profitieren'),\n",
            "((16, 17), 'profitieren'),\n",
            "((16, 17), '##ieren'),\n",
            "((16, 17), '##en'),\n",
            "((17, 23), 'nachdem von den Behörden empfohlen wurde'),\n",
            "((17, 23), 'nachdem von den Behörden empfohlen wurde ,'),\n",
            "((17, 23), '##dem von den Behörden empfohlen wurde'),\n",
            "((17, 23), '##m von den Behörden empfohlen wurde'),\n",
            "((18, 23), 'von den Behörden empfohlen wurde'),\n",
            "((19, 21), 'den Behörden'),\n",
            "((19, 23), 'den Behörden empfohlen wurde'),\n",
            "((20, 21), 'Behörden'),\n",
            "((20, 21), '##hörden'),\n",
            "((20, 21), '##örden'),\n",
            "((20, 21), '##rden'),\n",
            "((21, 22), 'empfohlen'),\n",
            "((21, 23), 'empfohlen wurde'),\n",
            "((21, 22), '##ohlen'),\n",
            "((21, 22), '##len'),\n",
            "((22, 23), 'w'),\n",
            "((22, 23), 'wurde'),\n",
            "((22, 23), '##ur'),\n",
            "((22, 23), '##urde'),\n",
            "((22, 23), '##de'),\n",
            "((23, 24), 'dies'),\n",
            "((23, 24), 'dieses'),\n",
            "((23, 26), 'dieses mehr Leuten'),\n",
            "((23, 27), 'dieses mehr Leuten für'),\n",
            "((23, 32), 'dieses mehr Leuten für einen längeren Zeitraum zu verabreichen'),\n",
            "((23, 32), 'dieses mehr Leuten für einen längeren Zeitraum zu verabreichen .'),\n",
            "((27, 32), 'einen längeren Zeitraum zu verabreichen'),\n",
            "((29, 32), 'Zeitraum zu verabreichen'),\n",
            "((30, 32), 'zu verabreichen'),\n",
            "((31, 32), 'verabreichen'),\n",
            "((31, 32), '##n'),\n",
            "((31, 32), '.'),\n",
            "((32, 37), 'Das gerinnungshemmende Medikament Ticagrelor reduziert'),\n",
            "((33, 37), 'gerinnungshemmende Medikament Ticagrelor reduziert'),\n",
            "((34, 37), 'Medikament Ticagrelor reduziert'),\n",
            "((35, 36), 'Ticagrelor'),\n",
            "((35, 37), 'Ticagrelor reduziert'),\n",
            "((36, 37), 'reduziert'),\n",
            "((36, 37), '##uziert'),\n",
            "((36, 37), '##iert'),\n",
            "((37, 46), 'das Risiko wiederholter Herzinfarkte für Personen mit einer Herzerkrankung'),\n",
            "((37, 46), 'das Risiko wiederholter Herzinfarkte für Personen mit einer Herzerkrankung .'),\n",
            "((38, 46), 'Risiko wiederholter Herzinfarkte für Personen mit einer Herzerkrankung'),\n",
            "((39, 46), 'wiederholter Herzinfarkte für Personen mit einer Herzerkrankung'),\n",
            "((39, 46), '##iederholter Herzinfarkte für Personen mit einer Herzerkrankung'),\n",
            "((39, 46), '##erholter Herzinfarkte für Personen mit einer Herzerkrankung'),\n",
            "((39, 46), '##ter Herzinfarkte für Personen mit einer Herzerkrankung'),\n",
            "((40, 46), 'Herzinfarkte für Personen mit einer Herzerkrankung'),\n",
            "((40, 46), '##kte für Personen mit einer Herzerkrankung'),\n",
            "((40, 46), '##e für Personen mit einer Herzerkrankung'),\n",
            "((41, 46), 'für Personen mit einer Herzerkrankung'),\n",
            "((42, 43), 'Personen'),\n",
            "((42, 46), 'Personen mit einer Herzerkrankung'),\n",
            "((43, 46), 'mit einer Herzerkrankung'),\n",
            "((44, 46), 'einer Herzerkrankung'),\n",
            "((45, 46), 'Her'),\n",
            "((45, 46), 'Herzerkrankung'),\n",
            "((45, 46), '##ung'),\n",
            "((45, 46), '.'),\n",
            "((46, 48), 'Das Medikament'),\n",
            "((46, 57), 'Das Medikament wird bereits 12 Monate lang nach einem Herzinfarkt verabreicht'),\n",
            "((48, 49), 'wird'),\n",
            "((48, 57), 'wird bereits 12 Monate lang nach einem Herzinfarkt verabreicht'),\n",
            "((48, 49), '##ird'),\n",
            "((48, 49), '##d'),\n",
            "((49, 57), 'bereits 12 Monate lang nach einem Herzinfarkt verabreicht'),\n",
            "((49, 57), '##reits 12 Monate lang nach einem Herzinfarkt verabreicht'),\n",
            "((50, 52), '12 Monate'),\n",
            "((50, 57), '12 Monate lang nach einem Herzinfarkt verabreicht'),\n",
            "((52, 53), 'lang'),\n",
            "((52, 57), 'lang nach einem Herzinfarkt verabreicht'),\n",
            "((53, 57), 'nach einem Herzinfarkt verabreicht'),\n",
            "((54, 57), 'einem Herzinfarkt verabreicht'),\n",
            "((54, 57), '##inem Herzinfarkt verabreicht'),\n",
            "((55, 56), 'Herzinfarkt'),\n",
            "((55, 57), 'Herzinfarkt verabreicht'),\n",
            "((56, 57), 'verabreicht'),\n",
            "((56, 57), '##rabreicht'),\n",
            "((56, 57), '##icht'),\n",
            "((57, 58), 'wodurch'),\n",
            "((58, 62), 'das Risiko eines Schlaganfalls'),\n",
            "((58, 68), 'das Risiko eines Schlaganfalls oder eines weiteren Herzinfarkts reduziert wird'),\n",
            "((58, 68), 'das Risiko eines Schlaganfalls oder eines weiteren Herzinfarkts reduziert wird .'),\n",
            "((60, 62), 'eines Schlaganfalls'),\n",
            "((62, 68), 'oder eines weiteren Herzinfarkts reduziert wird'),\n",
            "((62, 68), '##der eines weiteren Herzinfarkts reduziert wird'),\n",
            "((63, 68), 'eines weiteren Herzinfarkts reduziert wird'),\n",
            "((64, 65), 'weiteren'),\n",
            "((64, 68), 'weiteren Herzinfarkts reduziert wird'),\n",
            "((65, 66), 'Her'),\n",
            "((65, 67), 'Herzinfarkts reduziert'),\n",
            "((65, 68), 'Herzinfarkts reduziert wird'),\n",
            "((65, 67), '##farkts reduziert'),\n",
            "((66, 67), 'reduziert'),\n",
            "((66, 67), '##uziert'),\n",
            "((66, 67), '##iert'),\n",
            "((66, 67), '##t'),\n",
            "((67, 68), 'wird'),\n",
            "((67, 68), '##ird'),\n",
            "((67, 68), '##d'),\n",
            "((67, 68), '.'),\n",
            "((70, 71), 'NICE'),\n",
            "((70, 73), 'NICE des NHS'),\n",
            "((70, 80), 'NICE des NHS hat eine vier Jahre lange Einnahme empfohlen'),\n",
            "((72, 73), 'NHS'),\n",
            "((73, 80), 'hat eine vier Jahre lange Einnahme empfohlen'),\n",
            "((74, 80), 'eine vier Jahre lange Einnahme empfohlen'),\n",
            "((75, 80), 'vier Jahre lange Einnahme empfohlen'),\n",
            "((76, 80), 'Jahre lange Einnahme empfohlen'),\n",
            "((76, 80), '##re lange Einnahme empfohlen'),\n",
            "((77, 78), 'lange'),\n",
            "((77, 80), 'lange Einnahme empfohlen'),\n",
            "((78, 80), 'Einnahme empfohlen'),\n",
            "((78, 80), '##me empfohlen'),\n",
            "((79, 80), 'empfohlen'),\n",
            "((79, 80), '##pfohlen'),\n",
            "((79, 80), '##ohlen'),\n",
            "((79, 80), '##len'),\n",
            "((79, 80), ','),\n",
            "((79, 88), ', um das Risiko kardiovaskulärer Probleme weiter zu reduzieren .'),\n",
            "((80, 81), 'um'),\n",
            "((80, 88), 'um das Risiko kardiovaskulärer Probleme weiter zu reduzieren'),\n",
            "((80, 88), 'um das Risiko kardiovaskulärer Probleme weiter zu reduzieren .'),\n",
            "((81, 88), 'das Risiko kardiovaskulärer Probleme weiter zu reduzieren'),\n",
            "((85, 86), 'weiter'),\n",
            "((85, 87), 'weiter zu'),\n",
            "((85, 88), 'weiter zu reduzieren'),\n",
            "((87, 88), 'reduzieren'),\n",
            "((87, 88), '##ieren'),\n",
            "((87, 88), '##en'),\n",
            "((87, 88), '.'),\n",
            "((90, 91), 'NICE'),\n",
            "((90, 100), 'NICE des NHS hat eine vier Jahre lange Einnahme empfohlen'),\n",
            "((92, 93), 'NHS'),\n",
            "((94, 100), 'eine vier Jahre lange Einnahme empfohlen'),\n",
            "((95, 100), 'vier Jahre lange Einnahme empfohlen'),\n",
            "((96, 100), 'Jahre lange Einnahme empfohlen'),\n",
            "((97, 98), 'lange'),\n",
            "((97, 100), 'lange Einnahme empfohlen'),\n",
            "((98, 100), 'Einnahme empfohlen'),\n",
            "((99, 100), ','),\n",
            "((99, 108), ', um das Risiko kardiovaskulärer Probleme weiter zu reduzieren .'),\n",
            "((100, 101), 'um'),\n",
            "((100, 108), 'um das Risiko kardiovaskulärer Probleme weiter zu reduzieren .'),\n",
            "((101, 108), 'das Risiko kardiovaskulärer Probleme weiter zu reduzieren'),\n",
            "((101, 108), 'das Risiko kardiovaskulärer Probleme weiter zu reduzieren .'),\n",
            "((105, 107), 'weiter zu'),\n",
            "((105, 108), 'weiter zu reduzieren'),\n",
            "((107, 108), 'reduzieren'),\n",
            "((107, 108), '##ieren'),\n",
            "((107, 108), '##en'),\n",
            "((107, 108), '.'),\n",
            "((108, 109), 'Run'),\n",
            "((108, 109), 'Rund'),\n",
            "((108, 110), 'Rund 140 . 000'),\n",
            "((108, 111), 'Rund 140 . 000 Leute'),\n",
            "((108, 112), 'Rund 140 . 000 Leute erleiden'),\n",
            "((108, 119), 'Rund 140 . 000 Leute erleiden jedes Jahr einen Herzinfarkt und ein Viertel'),\n",
            "((109, 110), '140 . 000'),\n",
            "((109, 110), '. 000'),\n",
            "((109, 110), '000'),\n",
            "((110, 111), 'Leute'),\n",
            "((110, 111), '##ute'),\n",
            "((111, 112), 'erleiden'),\n",
            "((112, 119), 'jedes Jahr einen Herzinfarkt und ein Viertel'),\n",
            "((112, 119), '##edes Jahr einen Herzinfarkt und ein Viertel'),\n",
            "((113, 114), 'Jahr'),\n",
            "((113, 119), 'Jahr einen Herzinfarkt und ein Viertel'),\n",
            "((114, 116), 'einen Herzinfarkt'),\n",
            "((114, 117), 'einen Herzinfarkt und'),\n",
            "((114, 119), 'einen Herzinfarkt und ein Viertel'),\n",
            "((114, 116), '##inen Herzinfarkt'),\n",
            "((115, 116), 'Herzinfarkt'),\n",
            "((116, 117), 'und'),\n",
            "((117, 119), 'ein Viertel'),\n",
            "((118, 119), 'Viertel'),\n",
            "((118, 119), '##iertel'),\n",
            "((118, 119), '##tel'),\n",
            "((119, 120), 'da'),\n",
            "((119, 120), 'davon'),\n",
            "((119, 127), 'davon erleiden einen weiteren Herzinfarkt oder einen Schlaganfall .'),\n",
            "((119, 127), 'davon erleiden einen weiteren Herzinfarkt oder einen Schlaganfall . [SEP]'),\n",
            "((119, 120), '##von'),\n",
            "((120, 127), 'erleiden einen weiteren Herzinfarkt oder einen Schlaganfall .'),\n",
            "((121, 127), 'einen weiteren Herzinfarkt oder einen Schlaganfall .'),\n",
            "((122, 127), 'weiteren Herzinfarkt oder einen Schlaganfall .'),\n",
            "((123, 124), 'Herzinfarkt'),\n",
            "((123, 127), 'Herzinfarkt oder einen Schlaganfall'),\n",
            "((123, 127), 'Herzinfarkt oder einen Schlaganfall .'),\n",
            "((124, 125), 'oder'),\n",
            "((124, 127), 'oder einen Schlaganfall'),\n",
            "((125, 127), 'einen Schlaganfall'),\n",
            "((126, 127), 'Schlaganfall'),\n",
            "((126, 127), '.'),\n",
            "((126, 127), '[SEP]'),\n",
            "((127, 128), 'Her'),\n",
            "((130, 131), 'we'),\n",
            "((130, 131), 'werden'),\n",
            "((130, 132), 'werden durch'),\n",
            "((132, 135), 'Ansammlungen fetthaltigen Materials'),\n",
            "((132, 136), 'Ansammlungen fetthaltigen Materials in'),\n",
            "((132, 139), 'Ansammlungen fetthaltigen Materials in den Arterienwänden verursacht'),\n",
            "((132, 139), 'Ansammlungen fetthaltigen Materials in den Arterienwänden verursacht ,'),\n",
            "((132, 142), 'Ansammlungen fetthaltigen Materials in den Arterienwänden verursacht , die Beläge bilden'),\n",
            "((136, 139), 'den Arterienwänden verursacht'),\n",
            "((137, 139), 'Arterienwänden verursacht'),\n",
            "((137, 138), '##nden'),\n",
            "((138, 139), 'verursacht'),\n",
            "((138, 139), '##cht'),\n",
            "((138, 139), ','),\n",
            "((139, 142), 'die Beläge bilden'),\n",
            "((140, 141), 'Beläge'),\n",
            "((141, 142), '##n'),\n",
            "((142, 146), 'Wenn der Belag auseinanderbricht'),\n",
            "((142, 151), 'Wenn der Belag auseinanderbricht , kann es ein Blutgerinnsel verursachen'),\n",
            "((142, 151), 'Wenn der Belag auseinanderbricht , kann es ein Blutgerinnsel verursachen ,'),\n",
            "((143, 146), 'der Belag auseinanderbricht'),\n",
            "((144, 145), 'Belag'),\n",
            "((145, 146), 'auseinanderbricht'),\n",
            "((146, 147), 'kann'),\n",
            "((146, 151), 'kann es ein Blutgerinnsel verursachen'),\n",
            "((149, 151), 'Blutgerinnsel verursachen'),\n",
            "((151, 154), 'das den Blutfluss'),\n",
            "((151, 158), 'das den Blutfluss bis zum Herzen verstopft'),\n",
            "((151, 159), 'das den Blutfluss bis zum Herzen verstopft und'),\n",
            "((154, 158), 'bis zum Herzen verstopft'),\n",
            "((155, 158), 'zum Herzen verstopft'),\n",
            "((155, 158), '##m Herzen verstopft'),\n",
            "((156, 157), 'Herzen'),\n",
            "((156, 158), 'Herzen verstopft'),\n",
            "((156, 157), '##zen'),\n",
            "((157, 158), 'verstopft'),\n",
            "((157, 158), '##opft'),\n",
            "((157, 158), '##ft'),\n",
            "((159, 160), 'dadurch'),\n",
            "((159, 163), 'dadurch einen Herzinfarkt verursacht'),\n",
            "((160, 163), 'einen Herzinfarkt verursacht'),\n",
            "((161, 163), 'Herzinfarkt verursacht'),\n",
            "((163, 164), 'Wenn'),\n",
            "((163, 165), 'Wenn sich'),\n",
            "((163, 168), 'Wenn sich das Blutgerinnsel löst'),\n",
            "((165, 168), 'das Blutgerinnsel löst'),\n",
            "((168, 180), 'kann es durch den Blutstrom fließen und den Blutfluss zum Gehirn verstopfen'),\n",
            "((168, 180), 'kann es durch den Blutstrom fließen und den Blutfluss zum Gehirn verstopfen ,'),\n",
            "((171, 174), 'den Blutstrom fließen'),\n",
            "((171, 180), 'den Blutstrom fließen und den Blutfluss zum Gehirn verstopfen'),\n",
            "((173, 174), 'fließen'),\n",
            "((173, 174), '##n'),\n",
            "((175, 177), 'den Blutfluss'),\n",
            "((175, 180), 'den Blutfluss zum Gehirn verstopfen'),\n",
            "((177, 180), 'zum Gehirn verstopfen'),\n",
            "((178, 179), 'Gehirn'),\n",
            "((178, 180), 'Gehirn verstopfen'),\n",
            "((179, 180), 'verstopfen'),\n",
            "((179, 180), ','),\n",
            "((180, 181), 'was'),\n",
            "((180, 184), 'was einen Schlaganfall verursacht'),\n",
            "((180, 184), 'was einen Schlaganfall verursacht .'),\n",
            "((181, 184), 'einen Schlaganfall verursacht'),\n",
            "((182, 184), 'Schlaganfall verursacht'),\n",
            "((183, 184), '##cht'),\n",
            "((184, 185), 'Personen'),\n",
            "((184, 190), 'Personen , die bereits einen Herzinfarkt hatten'),\n",
            "((185, 190), 'die bereits einen Herzinfarkt hatten'),\n",
            "((187, 190), 'einen Herzinfarkt hatten'),\n",
            "((188, 190), 'Herzinfarkt hatten'),\n",
            "((189, 190), 'hatten'),\n",
            "((189, 190), '##ten'),\n",
            "((189, 198), ', unterliegen einem höheren Risiko einen weiteren zu haben .'),\n",
            "((190, 191), 'unterliegen'),\n",
            "((190, 198), 'unterliegen einem höheren Risiko einen weiteren zu haben'),\n",
            "((190, 198), 'unterliegen einem höheren Risiko einen weiteren zu haben .'),\n",
            "((191, 198), 'einem höheren Risiko einen weiteren zu haben'),\n",
            "((192, 198), 'höheren Risiko einen weiteren zu haben'),\n",
            "((192, 198), '##heren Risiko einen weiteren zu haben'),\n",
            "((193, 198), 'Risiko einen weiteren zu haben'),\n",
            "((194, 198), 'einen weiteren zu haben'),\n",
            "((194, 198), '##inen weiteren zu haben'),\n",
            "((195, 198), 'weiteren zu haben'),\n",
            "((196, 198), 'zu haben'),\n",
            "((197, 198), 'haben'),\n",
            "((204, 205), 'AstraZeneca'),\n",
            "((204, 214), 'AstraZeneca hergestellt wird und unter dem Handelsnamen Brilique vertrieben wird'),\n",
            "((205, 206), 'her'),\n",
            "((205, 206), 'herges'),\n",
            "((205, 206), 'hergestellt'),\n",
            "((205, 207), 'hergestellt wird'),\n",
            "((205, 214), 'hergestellt wird und unter dem Handelsnamen Brilique vertrieben wird'),\n",
            "((205, 206), '##tellt'),\n",
            "((205, 206), '##lt'),\n",
            "((206, 207), 'wird'),\n",
            "((206, 207), '##ird'),\n",
            "((206, 207), '##d'),\n",
            "((208, 214), 'unter dem Handelsnamen Brilique vertrieben wird'),\n",
            "((209, 213), 'dem Handelsnamen Brilique vertrieben'),\n",
            "((209, 214), 'dem Handelsnamen Brilique vertrieben wird'),\n",
            "((210, 213), 'Handelsnamen Brilique vertrieben'),\n",
            "((210, 213), '##namen Brilique vertrieben'),\n",
            "((211, 212), 'Brilique'),\n",
            "((211, 213), 'Brilique vertrieben'),\n",
            "((212, 213), 'vertrieben'),\n",
            "((213, 214), 'wird'),\n",
            "((213, 214), '##ird'),\n",
            "((213, 214), '##d'),\n",
            "((214, 217), 'reduziert dieses Risiko'),\n",
            "((215, 217), 'dieses Risiko'),\n",
            "((216, 217), ','),\n",
            "((216, 225), ', indem es die Bildung von Blutgerinnseln unwahrscheinlicher macht .'),\n",
            "((217, 225), 'indem es die Bildung von Blutgerinnseln unwahrscheinlicher macht'),\n",
            "((217, 225), 'indem es die Bildung von Blutgerinnseln unwahrscheinlicher macht .'),\n",
            "((219, 225), 'die Bildung von Blutgerinnseln unwahrscheinlicher macht'),\n",
            "((222, 223), 'Blutgerinnseln'),\n",
            "((222, 225), 'Blutgerinnseln unwahrscheinlicher macht'),\n",
            "((223, 225), 'unwahrscheinlicher macht'),\n",
            "((224, 225), '.'),\n",
            "((225, 229), 'Der Anleitungsentwurf von NICE'),\n",
            "((225, 233), 'Der Anleitungsentwurf von NICE , der heute veröffentlicht wurde'),\n",
            "((226, 229), 'Anleitungsentwurf von NICE'),\n",
            "((228, 229), 'NICE'),\n",
            "((229, 233), 'der heute veröffentlicht wurde'),\n",
            "((230, 233), 'heute veröffentlicht wurde'),\n",
            "((230, 233), '##ute veröffentlicht wurde'),\n",
            "((231, 233), 'veröffentlicht wurde'),\n",
            "((231, 233), '##öffentlicht wurde'),\n",
            "((231, 233), '##ffentlicht wurde'),\n",
            "((231, 233), '##ntlicht wurde'),\n",
            "((231, 233), '##licht wurde'),\n",
            "((232, 233), 'wurde'),\n",
            "((232, 233), '##urde'),\n",
            "((232, 233), '##de'),\n",
            "((233, 240), 'empfiehlt eine 12 - monatige Einnahme von 90mg Ticagrelor'),\n",
            "((233, 240), 'empfiehlt eine 12 - monatige Einnahme von 90mg Ticagrelor ,'),\n",
            "((234, 240), 'eine 12 - monatige Einnahme von 90mg Ticagrelor'),\n",
            "((238, 240), '90mg Ticagrelor'),\n",
            "((240, 243), 'gefolgt von 60mg'),\n",
            "((242, 243), '60mg'),\n",
            "((243, 255), 'mit einer zweimal täglichen Einnahme von Aspirin für die nächsten drei Jahre'),\n",
            "((243, 255), 'mit einer zweimal täglichen Einnahme von Aspirin für die nächsten drei Jahre .'),\n",
            "((245, 255), 'zweimal täglichen Einnahme von Aspirin für die nächsten drei Jahre'),\n",
            "((246, 255), 'täglichen Einnahme von Aspirin für die nächsten drei Jahre'),\n",
            "((246, 255), '##chen Einnahme von Aspirin für die nächsten drei Jahre'),\n",
            "((247, 255), 'Einnahme von Aspirin für die nächsten drei Jahre'),\n",
            "((247, 255), '##me von Aspirin für die nächsten drei Jahre'),\n",
            "((249, 255), 'Aspirin für die nächsten drei Jahre'),\n",
            "((250, 255), 'für die nächsten drei Jahre'),\n",
            "((251, 255), 'die nächsten drei Jahre'),\n",
            "((254, 255), '##re'),\n",
            "((258, 263), 'Direktorin des NICE Gesundheitstechnologieevaluierungszentrums sagte'),\n",
            "((260, 261), 'NICE'),\n",
            "((260, 262), 'NICE Gesundheitstechnologieevaluierungszentrums'),\n",
            "((260, 263), 'NICE Gesundheitstechnologieevaluierungszentrums sagt'),\n",
            "((260, 263), 'NICE Gesundheitstechnologieevaluierungszentrums sagte'),\n",
            "((261, 262), 'Gesundheitstechnologieevaluierungszentrums'),\n",
            "((262, 263), '##e'),\n",
            "((263, 264), '\"'),\n",
            "((264, 273), 'der Verfügbarkeit von Sekundärprävention haben ein Viertel aller Personen'),\n",
            "((267, 268), 'Sekundärprävention'),\n",
            "((267, 273), 'Sekundärprävention haben ein Viertel aller Personen'),\n",
            "((267, 268), '##ndärprävention'),\n",
            "((268, 269), 'haben'),\n",
            "((268, 273), 'haben ein Viertel aller Personen'),\n",
            "((269, 273), 'ein Viertel aller Personen'),\n",
            "((270, 273), 'Viertel aller Personen'),\n",
            "((271, 273), 'aller Personen'),\n",
            "((273, 278), 'die einen Herzinfarkt erlitten haben'),\n",
            "((275, 276), 'Herzinfarkt'),\n",
            "((275, 278), 'Herzinfarkt erlitten haben'),\n",
            "((278, 281), 'einen weiteren Herzinfarkt'),\n",
            "((278, 289), 'einen weiteren Herzinfarkt oder einen Schlaganfall - oft mit desaströsen Folgen'),\n",
            "((278, 289), 'einen weiteren Herzinfarkt oder einen Schlaganfall - oft mit desaströsen Folgen .'),\n",
            "((280, 281), 'Herzinfarkt'),\n",
            "((281, 289), 'oder einen Schlaganfall - oft mit desaströsen Folgen'),\n",
            "((293, 294), 'Herzinfarkt'),\n",
            "((293, 304), 'Herzinfarkt kann erhebliche negative Auswirkungen auf die Lebensqualität einer Person haben'),\n",
            "((294, 304), 'kann erhebliche negative Auswirkungen auf die Lebensqualität einer Person haben'),\n",
            "((298, 304), 'auf die Lebensqualität einer Person haben'),\n",
            "((299, 304), 'die Lebensqualität einer Person haben'),\n",
            "((304, 307), 'Die Erfahrung zeigt'),\n",
            "((306, 307), 'zeigt'),\n",
            "((310, 311), 'Kombination'),\n",
            "((312, 313), 'Aspirin'),\n",
            "((314, 324), 'bei der Reduzierung weiterer Herzinfarkte und Schlaganfälle bei Leuten ist'),\n",
            "((314, 324), 'bei der Reduzierung weiterer Herzinfarkte und Schlaganfälle bei Leuten ist ,'),\n",
            "((315, 321), 'der Reduzierung weiterer Herzinfarkte und Schlaganfälle'),\n",
            "((315, 324), 'der Reduzierung weiterer Herzinfarkte und Schlaganfälle bei Leuten ist'),\n",
            "((318, 319), 'Herzinfarkte'),\n",
            "((318, 321), 'Herzinfarkte und Schlaganfälle'),\n",
            "((320, 321), 'Schlaganfälle'),\n",
            "((321, 323), 'bei Leuten'),\n",
            "((321, 324), 'bei Leuten ist'),\n",
            "((322, 323), 'Leuten'),\n",
            "((322, 323), '##uten'),\n",
            "((323, 324), 'is'),\n",
            "((323, 324), 'ist'),\n",
            "((324, 329), 'die bereits einen Herzinfarkt hatten'),\n",
            "((324, 329), 'die bereits einen Herzinfarkt hatten .'),\n",
            "((327, 329), 'Herzinfarkt hatten'),\n",
            "((328, 329), '##ten'),\n",
            "((329, 335), 'Durch eine vorläufige Empfehlung von Ticagrelor'),\n",
            "((329, 338), 'Durch eine vorläufige Empfehlung von Ticagrelor freuen wir uns'),\n",
            "((330, 335), 'eine vorläufige Empfehlung von Ticagrelor'),\n",
            "((334, 335), 'Ticagrelor'),\n",
            "((335, 336), 'freuen'),\n",
            "((335, 338), 'freuen wir uns'),\n",
            "((335, 336), '##uen'),\n",
            "((336, 338), 'wir uns'),\n",
            "((338, 344), 'dass wir in der Lage sind'),\n",
            "((341, 344), 'der Lage sind'),\n",
            "((342, 343), 'Lage'),\n",
            "((343, 344), 'sind'),\n",
            "((344, 352), 'verfügbare Behandlungsoptionen an tausende von Menschen zu erweitern'),\n",
            "((344, 352), 'verfügbare Behandlungsoptionen an tausende von Menschen zu erweitern ,'),\n",
            "((344, 352), '##ügbare Behandlungsoptionen an tausende von Menschen zu erweitern'),\n",
            "((345, 352), 'Behandlungsoptionen an tausende von Menschen zu erweitern'),\n",
            "((346, 352), 'an tausende von Menschen zu erweitern'),\n",
            "((347, 352), '##nde von Menschen zu erweitern'),\n",
            "((348, 352), 'von Menschen zu erweitern'),\n",
            "((349, 352), 'Menschen zu erweitern'),\n",
            "((351, 352), 'erweitern'),\n",
            "((351, 352), ','),\n",
            "((352, 356), 'die davon profitieren können'),\n",
            "((352, 356), 'die davon profitieren können .'),\n",
            "((352, 356), 'die davon profitieren können . [SEP]'),\n",
            "((355, 356), '##n'),\n",
            "((356, 365), 'Die Information über die Wirksamkeit und Sicherheit von Ticagrelor'),\n",
            "((359, 365), 'die Wirksamkeit und Sicherheit von Ticagrelor'),\n",
            "((371, 381), 'ist auf einen Zeitraum von bis zu drei Jahren beschränkt .'),\n",
            "((373, 381), 'einen Zeitraum von bis zu drei Jahren beschränkt'),\n",
            "((381, 386), 'Der Anleitungsentwurf empfiehlt keine Behandlung'),\n",
            "((386, 391), 'die über diesen Zeitraum hinausgeht'),\n",
            "((386, 391), 'die über diesen Zeitraum hinausgeht .'),\n",
            "((390, 391), '.'),\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efiwsb7-83q8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TDuZRuZVeKX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNyPDWS1VeNg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNPiKtrgFj2O"
      },
      "source": [
        "# MLQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Hrmmn3HczyC",
        "outputId": "ea2d5176-6305-4ec4-a395-40e45849ee2c"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/MLQA.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MLQA'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 19 (delta 8), reused 15 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ifBcdUc1QL",
        "outputId": "7af44b67-5f53-48e0-dfd2-efd5025b4888"
      },
      "source": [
        "! python ./MLQA/mlqa_evaluation_v1.py ./dev-context-en-question-en.json ./english_predictions.json en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"./MLQA/mlqa_evaluation_v1.py\", line 152, in <module>\n",
            "    predictions = json.load(prediction_file)\n",
            "  File \"/usr/lib/python3.7/json/__init__.py\", line 296, in load\n",
            "    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
            "  File \"/usr/lib/python3.7/json/__init__.py\", line 348, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.7/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.7/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uR6fDQ9dPy8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMiqW4NdTNWz"
      },
      "source": [
        "# https://www.kaggle.com/kashnitsky/simple-logistic-regression-bert-0-27-lb BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOeXFupATOYC"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FBrlwhrKT0td",
        "outputId": "e69b8325-a888-4253-9529-e8e2b3d453ca"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/72/d06017379ad4760dc58781c765376ce4ba5dcf3c08d37032eeefbccf1c51/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.12.4)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.32.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15) (54.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=5010efdeae0a5b11e920f1462e851467b7e1fbf112be0c9d6484e7efec3c0f7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, keras-applications, tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MmoJpC-m_R7"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfCroP-4TRsV"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n",
        "!wget -q https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n",
        "!wget -q https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\n",
        "!wget -q https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
        "!wget -q https://raw.githubusercontent.com/google-research/bert/master/extract_features.py \n",
        "!wget -q https://raw.githubusercontent.com/google-research/bert/master/tokenization.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjv_oZTtitiu"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/google-research/bert/master/extract_features.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8AMHu8OTVUl"
      },
      "source": [
        "import modeling\n",
        "import extract_features\n",
        "import tokenization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-AbSoUyTbxh"
      },
      "source": [
        "val_df = pd.read_table('gap-validation.tsv', index_col='ID').reset_index(drop=True)\n",
        "test_df  = pd.read_table('gap-validation.tsv', index_col='ID').reset_index(drop=True)\n",
        "dev_df  = pd.read_table('gap-development.tsv', index_col='ID').reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDfT4M-PUxmd",
        "outputId": "61b4c442-8f33-420f-b665-4a16b933f36a"
      },
      "source": [
        "!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip \n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQZ3fUBxUzEQ"
      },
      "source": [
        "def count_char(text, offset):   \n",
        "    count = 0\n",
        "    for pos in range(offset):\n",
        "        if text[pos] != \" \": count +=1\n",
        "    return count\n",
        "\n",
        "def candidate_length(candidate):\n",
        "    count = 0\n",
        "    for i in range(len(candidate)):\n",
        "        if candidate[i] !=  \" \": count += 1\n",
        "    return count\n",
        "\n",
        "def count_token_length_special(token):\n",
        "    count = 0\n",
        "    special_token = [\"#\", \" \"]\n",
        "    for i in range(len(token)):\n",
        "        if token[i] not in special_token: count+=1\n",
        "    return count\n",
        "\n",
        "def embed_by_bert(df, path_to_bert='uncased_L-12_H-768_A-12', embed_size=768, batch_size=8,\n",
        "                 layers='-1', max_seq_length=256):\n",
        "    \n",
        "    text = df['Text']\n",
        "    text.to_csv('input.txt', index=False, header=False)\n",
        "    os.system(f\"python3 extract_features.py \\\n",
        "               --input_file=input.txt \\\n",
        "               --output_file=output.jsonl \\\n",
        "               --vocab_file={path_to_bert}/vocab.txt \\\n",
        "               --bert_config_file={path_to_bert}/bert_config.json \\\n",
        "               --init_checkpoint={path_to_bert}/bert_model.ckpt \\\n",
        "               --layers={layers} \\\n",
        "               --max_seq_length={max_seq_length} \\\n",
        "               --batch_size={batch_size}\")\n",
        "    \n",
        "    bert_output = pd.read_json(\"output.jsonl\", lines=True)\n",
        "    # print(bert_output.head())\n",
        "    \n",
        "    # os.system(\"rm input.txt\")\n",
        "    # os.system(\"rm output.jsonl\")\n",
        "    \n",
        "    index = df.index\n",
        "    columns = [\"emb_A\", \"emb_B\", \"emb_P\", \"label\"]\n",
        "    emb = pd.DataFrame(index = index, columns = columns)\n",
        "    emb.index.name = \"ID\"\n",
        "    \n",
        "    for i in tqdm(range(len(text))):\n",
        "        \n",
        "        features = bert_output.loc[i, \"features\"]\n",
        "        P_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'Pronoun-offset'])\n",
        "        A_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'A-offset'])\n",
        "        B_char_start = count_char(df.loc[i, 'Text'], df.loc[i, 'B-offset'])\n",
        "        A_length = candidate_length(df.loc[i, 'A'])\n",
        "        B_length = candidate_length(df.loc[i, 'B'])\n",
        "        \n",
        "        emb_A, emb_B, emb_P = np.zeros(embed_size), np.zeros(embed_size), np.zeros(embed_size)\n",
        "        char_count, cnt_A, cnt_B = 0, 0, 0\n",
        "        \n",
        "        for j in range(2, len(features)):\n",
        "            token = features[j][\"token\"]\n",
        "            token_length = count_token_length_special(token)\n",
        "            if char_count == P_char_start:\n",
        "                emb_P += np.asarray(features[j][\"layers\"][0]['values']) \n",
        "            if char_count in range(A_char_start, A_char_start + A_length):\n",
        "                emb_A += np.asarray(features[j][\"layers\"][0]['values'])\n",
        "                cnt_A += 1\n",
        "            if char_count in range(B_char_start, B_char_start + B_length):\n",
        "                emb_B += np.asarray(features[j][\"layers\"][0]['values'])\n",
        "                cnt_B += 1                \n",
        "            char_count += token_length\n",
        "        \n",
        "        if cnt_A > 0:\n",
        "            emb_A /= cnt_A\n",
        "        if cnt_B > 0:\n",
        "            emb_B /= cnt_B\n",
        "        \n",
        "        label = \"Neither\"\n",
        "        if (df.loc[i,\"A-coref\"] == True):\n",
        "            label = \"A\"\n",
        "        if (df.loc[i,\"B-coref\"] == True):\n",
        "            label = \"B\"\n",
        "\n",
        "        emb.iloc[i] = [emb_A, emb_B, emb_P, label]\n",
        "        \n",
        "    return emb    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCI-9RoBVEcF",
        "outputId": "c7b4a06b-eeb1-4b32-8f07-182831513b2a"
      },
      "source": [
        "%time\n",
        "val_bert_emb = embed_by_bert(val_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 5.48 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/454 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   linex_index                                           features\n",
            "0            0  [{'token': '[CLS]', 'layers': [{'index': -1, '...\n",
            "1            1  [{'token': '[CLS]', 'layers': [{'index': -1, '...\n",
            "2            2  [{'token': '[CLS]', 'layers': [{'index': -1, '...\n",
            "3            3  [{'token': '[CLS]', 'layers': [{'index': -1, '...\n",
            "4            4  [{'token': '[CLS]', 'layers': [{'index': -1, '...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py:849: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  arr_value = np.array(value)\n",
            "100%|██████████| 454/454 [00:00<00:00, 1343.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVU1p8ZIfYN1",
        "outputId": "97f343a2-b1dc-464f-e6e2-4d887a55775a"
      },
      "source": [
        "test_bert_emb = embed_by_bert(test_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/454 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py:849: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  arr_value = np.array(value)\n",
            "100%|██████████| 454/454 [00:00<00:00, 1410.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aakMpcDkfaAd",
        "outputId": "0c584e77-6039-4610-95b4-d098054ed835"
      },
      "source": [
        "%time\n",
        "dev_bert_emb = embed_by_bert(dev_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.15 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py:849: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  arr_value = np.array(value)\n",
            "100%|██████████| 2000/2000 [00:01<00:00, 1398.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8wpjVl6Xrw5",
        "outputId": "9622b960-67ff-4209-ac68-4ea58e43cee3"
      },
      "source": [
        "val_bert_emb[\"emb_A\"].head().map(np.asarray).values[0].astype('float').shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCCYfJJ0fT0z"
      },
      "source": [
        "def featurize(embedding_df):\n",
        "    \n",
        "    pronoun_embs, a_embs, b_embs, labels = [], [], [], []\n",
        "    \n",
        "    for i in tqdm(range(len(embedding_df))):\n",
        "        \n",
        "        pronoun_embs.append(embedding_df.loc[i, \"emb_P\"])\n",
        "        a_embs.append(embedding_df.loc[i, \"emb_A\"])\n",
        "        b_embs.append(embedding_df.loc[i, \"emb_B\"])\n",
        "\n",
        "        label_map = {'A': 0, 'B': 1, 'Neither': 2}\n",
        "        labels.append(label_map[embedding_df.loc[i, \"label\"]])\n",
        "\n",
        "    \n",
        "    a_embs = np.asarray(a_embs).astype('float')\n",
        "    b_embs = np.asarray(b_embs).astype('float') \n",
        "    pronoun_embs = np.asarray(pronoun_embs).astype('float')\n",
        "    \n",
        "    return np.concatenate([a_embs, b_embs, pronoun_embs], axis=1), np.asarray(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwztiQWvfz4Q",
        "outputId": "e2c1d0ab-3853-4807-c6bd-caca37587385"
      },
      "source": [
        "X_train, y_train = featurize(pd.concat([val_bert_emb, dev_bert_emb]).sort_index().reset_index())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2454/2454 [00:00<00:00, 36265.70it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHcPuL_Bf0Ev",
        "outputId": "18f91645-84c7-433e-9fd5-0225141179eb"
      },
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2454, 2304), (2454,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZKyu-2Of0XI"
      },
      "source": [
        "logit = LogisticRegression(C=1e-2, random_state=17, solver='lbfgs', \n",
        "                           multi_class='multinomial', max_iter=100,\n",
        "                          n_jobs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnYxjdzFf0hA",
        "outputId": "79d9b8aa-c43a-4d3b-d9d6-3d4f0c3d8fdb"
      },
      "source": [
        "%%time\n",
        "logit.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 65.9 ms, sys: 504 ms, total: 570 ms\n",
            "Wall time: 5.19 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='multinomial', n_jobs=4, penalty='l2',\n",
              "                   random_state=17, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDiE2_jQgEbR"
      },
      "source": [
        "!cp gap-development.tsv stage1_test.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyC-b9mxgEgD"
      },
      "source": [
        "stage1_test_df  = pd.read_table('stage1_test.tsv', index_col='ID').reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eFR_cYWgEkA",
        "outputId": "0e782d9c-6f37-48bf-9bed-5a3bdfdff9a3"
      },
      "source": [
        "%time\n",
        "stage1_test_bert_emb = embed_by_bert(stage1_test_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 7.87 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py:849: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  arr_value = np.array(value)\n",
            "100%|██████████| 2000/2000 [00:01<00:00, 1331.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEq_ShmQgEn5",
        "outputId": "d986c4b0-c144-464f-8310-e6d326ba3d0f"
      },
      "source": [
        "X_test, y_test = featurize(stage1_test_bert_emb)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:00<00:00, 34504.69it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whn129TNgEqr",
        "outputId": "c0a6bdc8-b067-4e90-e40d-c0d7779ff79e"
      },
      "source": [
        "logit_test_pred = logit.predict_proba(X_test)\n",
        "log_loss(y_test, logit_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.27757309888421444"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW_5pu4dgEtw"
      },
      "source": [
        "# Write the prediction to file for submission\n",
        "# submission = pd.read_csv(\"../input/sample_submission_stage_1.csv\", index_col = \"ID\")\n",
        "submission = dict()\n",
        "A = logit_test_pred[:, 0]\n",
        "B = logit_test_pred[:, 1]\n",
        "N = logit_test_pred[:, 2]\n",
        "# submission.to_csv(\"submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lZniCgbgEwz"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXVQW3oSo1sc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3Sh6mDCgE0C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4EZuD4AgE3e",
        "outputId": "41d9c267-dae5-4977-d26d-e701ba9d4e0b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.94      0.95      0.94       874\n",
            "           B       0.93      0.95      0.94       925\n",
            "           N       0.98      0.80      0.88       201\n",
            "\n",
            "    accuracy                           0.94      2000\n",
            "   macro avg       0.95      0.90      0.92      2000\n",
            "weighted avg       0.94      0.94      0.93      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZiNHQLSgFDD",
        "outputId": "9d32962e-bf97-4573-d8d1-12bcfea68057"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pbbkvjRf0t4",
        "outputId": "c597e754-1fcb-4512-e3f7-f152a704ec68"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "val_df = pd.read_table('gap-validation.tsv', index_col='ID').reset_index(drop=True)\n",
        "test_df  = pd.read_table('gap-test.tsv', index_col='ID').reset_index(drop=True)\n",
        "dev_df  = pd.read_table('gap-development.tsv', index_col='ID').reset_index(drop=True)\n",
        "val_bert_emb = embed_by_bert(val_df)\n",
        "\n",
        "dev_bert_emb = embed_by_bert(dev_df)\n",
        "X_train, y_train = featurize(pd.concat([val_bert_emb, dev_bert_emb]).sort_index().reset_index())\n",
        "logit = LogisticRegression(C=1e-2, random_state=17, solver='lbfgs', \n",
        "                           multi_class='multinomial', max_iter=100,\n",
        "                          n_jobs=4)\n",
        "logit.fit(X_train, y_train)\n",
        "test_bert_emb = embed_by_bert(test_df)\n",
        "X_test, y_test = featurize(test_bert_emb)\n",
        "logit_test_pred = logit.predict_proba(X_test)\n",
        "log_loss(y_test, logit_test_pred)\n",
        "submission = dict()\n",
        "A = logit_test_pred[:, 0]\n",
        "B = logit_test_pred[:, 1]\n",
        "N = logit_test_pred[:, 2]\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/454 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py:849: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  arr_value = np.array(value)\n",
            "100%|██████████| 454/454 [00:00<00:00, 1136.44it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py:849: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  arr_value = np.array(value)\n",
            "100%|██████████| 2000/2000 [00:01<00:00, 1098.85it/s]\n",
            "100%|██████████| 2454/2454 [00:00<00:00, 25799.39it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py:849: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  arr_value = np.array(value)\n",
            "100%|██████████| 2000/2000 [00:01<00:00, 1116.46it/s]\n",
            "100%|██████████| 2000/2000 [00:00<00:00, 26711.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "591.6894416809082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A9MSkmwueW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf2886b-84ad-41aa-da0a-827379622765"
      },
      "source": [
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "591.6894416809082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJaHxo-zqeyC"
      },
      "source": [
        "preds = []\n",
        "for i in range(2000):\n",
        "    if A[i] > B[i] and A[i] > N[i]:\n",
        "        preds.append('A')\n",
        "    elif B[i] > A[i] and B[i] > N[i]:\n",
        "        preds.append('B')\n",
        "    else:\n",
        "        preds.append('N')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdpc6h1kndjg"
      },
      "source": [
        "dev_df = pd.read_csv('./gap-test.tsv', delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHbGVwloqjpi"
      },
      "source": [
        "act = []\n",
        "for i, row in dev_df.iterrows():\n",
        "    if row['A-coref']:\n",
        "        act.append('A')\n",
        "    elif row['B-coref']:\n",
        "        act.append('B')\n",
        "    else:\n",
        "        act.append('N')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo4v4bQJqjwr",
        "outputId": "f9587b49-9afe-4434-e3d2-c2fba3abc4e2"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(act, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.81      0.81      0.81       918\n",
            "           B       0.74      0.85      0.79       855\n",
            "           N       0.73      0.33      0.45       227\n",
            "\n",
            "    accuracy                           0.77      2000\n",
            "   macro avg       0.76      0.66      0.68      2000\n",
            "weighted avg       0.77      0.77      0.76      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEhpObrVqj1e",
        "outputId": "13e72b57-b372-48d1-c561-405a85a7d33d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(act, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8fUxeuvQZ_O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}